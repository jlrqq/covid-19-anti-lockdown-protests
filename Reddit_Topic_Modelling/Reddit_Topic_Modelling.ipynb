{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796b7b16",
   "metadata": {},
   "source": [
    "<h1>Reddit Scraped Comments</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b10d14b",
   "metadata": {},
   "source": [
    "<h3>Installation and import of libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69af6421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\lindy\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.22.4)\n",
      "Requirement already satisfied: pandas>=1.3.4 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.4)\n",
      "Requirement already satisfied: funcy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.18)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (58.0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: numexpr in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.7.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.3.4->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c3ae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lindy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords') #download if don't have yet\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer # tokenize words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b59e22",
   "metadata": {},
   "source": [
    "<h3>Viewing Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44eafe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "folder_path = \"../Reddit_Scraped_Comments/\"\n",
    "file1 = \"anti_lockdown_comments.csv\"\n",
    "file2 = \"corona_lockdown_comments.csv\"\n",
    "file3 = \"covid19_lockdown_comments.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1c9604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>commenter</th>\n",
       "      <th>comment</th>\n",
       "      <th>top_lvl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ij071t</td>\n",
       "      <td>casualphilosopher1</td>\n",
       "      <td>10,000 anti-lockdown protesters gather in Lond...</td>\n",
       "      <td>3039</td>\n",
       "      <td>600</td>\n",
       "      <td>1.598735e+09</td>\n",
       "      <td>2020-08-30 05:05:25</td>\n",
       "      <td>schu4KSU</td>\n",
       "      <td>People like this everywhere in the world.  Dif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ij071t</td>\n",
       "      <td>casualphilosopher1</td>\n",
       "      <td>10,000 anti-lockdown protesters gather in Lond...</td>\n",
       "      <td>3039</td>\n",
       "      <td>600</td>\n",
       "      <td>1.598735e+09</td>\n",
       "      <td>2020-08-30 05:05:25</td>\n",
       "      <td>StupidizeMe</td>\n",
       "      <td>Well, there's certainly enough of them to caus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ij071t</td>\n",
       "      <td>casualphilosopher1</td>\n",
       "      <td>10,000 anti-lockdown protesters gather in Lond...</td>\n",
       "      <td>3039</td>\n",
       "      <td>600</td>\n",
       "      <td>1.598735e+09</td>\n",
       "      <td>2020-08-30 05:05:25</td>\n",
       "      <td>Eltharion-the-Grim</td>\n",
       "      <td>They are largely absent from Asia. The only pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ij071t</td>\n",
       "      <td>casualphilosopher1</td>\n",
       "      <td>10,000 anti-lockdown protesters gather in Lond...</td>\n",
       "      <td>3039</td>\n",
       "      <td>600</td>\n",
       "      <td>1.598735e+09</td>\n",
       "      <td>2020-08-30 05:05:25</td>\n",
       "      <td>Thisam</td>\n",
       "      <td>Yup, the percentage of population who are easi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ij071t</td>\n",
       "      <td>casualphilosopher1</td>\n",
       "      <td>10,000 anti-lockdown protesters gather in Lond...</td>\n",
       "      <td>3039</td>\n",
       "      <td>600</td>\n",
       "      <td>1.598735e+09</td>\n",
       "      <td>2020-08-30 05:05:25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Short answer: Yes. Look at the USA and how Bre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0</td>\n",
       "      <td>g4kdfs</td>\n",
       "      <td>vostok-Abdullah</td>\n",
       "      <td>Counter-Protesters in Scrubs Block Some Anti-L...</td>\n",
       "      <td>1483</td>\n",
       "      <td>192</td>\n",
       "      <td>1.587349e+09</td>\n",
       "      <td>2020-04-20 10:16:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0</td>\n",
       "      <td>g4kdfs</td>\n",
       "      <td>vostok-Abdullah</td>\n",
       "      <td>Counter-Protesters in Scrubs Block Some Anti-L...</td>\n",
       "      <td>1483</td>\n",
       "      <td>192</td>\n",
       "      <td>1.587349e+09</td>\n",
       "      <td>2020-04-20 10:16:05</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Your comment has been removed because\\n\\n* **I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0</td>\n",
       "      <td>g4kdfs</td>\n",
       "      <td>vostok-Abdullah</td>\n",
       "      <td>Counter-Protesters in Scrubs Block Some Anti-L...</td>\n",
       "      <td>1483</td>\n",
       "      <td>192</td>\n",
       "      <td>1.587349e+09</td>\n",
       "      <td>2020-04-20 10:16:05</td>\n",
       "      <td>Ameriican</td>\n",
       "      <td>They broke quarintine to tell others to not br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>0</td>\n",
       "      <td>g4kdfs</td>\n",
       "      <td>vostok-Abdullah</td>\n",
       "      <td>Counter-Protesters in Scrubs Block Some Anti-L...</td>\n",
       "      <td>1483</td>\n",
       "      <td>192</td>\n",
       "      <td>1.587349e+09</td>\n",
       "      <td>2020-04-20 10:16:05</td>\n",
       "      <td>AshingiiAshuaa</td>\n",
       "      <td>If the protesters' goal is to clog up streets ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0</td>\n",
       "      <td>g4kdfs</td>\n",
       "      <td>vostok-Abdullah</td>\n",
       "      <td>Counter-Protesters in Scrubs Block Some Anti-L...</td>\n",
       "      <td>1483</td>\n",
       "      <td>192</td>\n",
       "      <td>1.587349e+09</td>\n",
       "      <td>2020-04-20 10:16:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They are blocking the protesters from blocking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0      id              author  \\\n",
       "0             0  ij071t  casualphilosopher1   \n",
       "1             0  ij071t  casualphilosopher1   \n",
       "2             0  ij071t  casualphilosopher1   \n",
       "3             0  ij071t  casualphilosopher1   \n",
       "4             0  ij071t  casualphilosopher1   \n",
       "..          ...     ...                 ...   \n",
       "705           0  g4kdfs     vostok-Abdullah   \n",
       "706           0  g4kdfs     vostok-Abdullah   \n",
       "707           0  g4kdfs     vostok-Abdullah   \n",
       "708           0  g4kdfs     vostok-Abdullah   \n",
       "709           0  g4kdfs     vostok-Abdullah   \n",
       "\n",
       "                                                 title  score  comms_num  \\\n",
       "0    10,000 anti-lockdown protesters gather in Lond...   3039        600   \n",
       "1    10,000 anti-lockdown protesters gather in Lond...   3039        600   \n",
       "2    10,000 anti-lockdown protesters gather in Lond...   3039        600   \n",
       "3    10,000 anti-lockdown protesters gather in Lond...   3039        600   \n",
       "4    10,000 anti-lockdown protesters gather in Lond...   3039        600   \n",
       "..                                                 ...    ...        ...   \n",
       "705  Counter-Protesters in Scrubs Block Some Anti-L...   1483        192   \n",
       "706  Counter-Protesters in Scrubs Block Some Anti-L...   1483        192   \n",
       "707  Counter-Protesters in Scrubs Block Some Anti-L...   1483        192   \n",
       "708  Counter-Protesters in Scrubs Block Some Anti-L...   1483        192   \n",
       "709  Counter-Protesters in Scrubs Block Some Anti-L...   1483        192   \n",
       "\n",
       "          created            timestamp           commenter  \\\n",
       "0    1.598735e+09  2020-08-30 05:05:25            schu4KSU   \n",
       "1    1.598735e+09  2020-08-30 05:05:25         StupidizeMe   \n",
       "2    1.598735e+09  2020-08-30 05:05:25  Eltharion-the-Grim   \n",
       "3    1.598735e+09  2020-08-30 05:05:25              Thisam   \n",
       "4    1.598735e+09  2020-08-30 05:05:25                 NaN   \n",
       "..            ...                  ...                 ...   \n",
       "705  1.587349e+09  2020-04-20 10:16:05                 NaN   \n",
       "706  1.587349e+09  2020-04-20 10:16:05       AutoModerator   \n",
       "707  1.587349e+09  2020-04-20 10:16:05           Ameriican   \n",
       "708  1.587349e+09  2020-04-20 10:16:05      AshingiiAshuaa   \n",
       "709  1.587349e+09  2020-04-20 10:16:05                 NaN   \n",
       "\n",
       "                                               comment  top_lvl  \n",
       "0    People like this everywhere in the world.  Dif...        1  \n",
       "1    Well, there's certainly enough of them to caus...        0  \n",
       "2    They are largely absent from Asia. The only pl...        0  \n",
       "3    Yup, the percentage of population who are easi...        0  \n",
       "4    Short answer: Yes. Look at the USA and how Bre...        0  \n",
       "..                                                 ...      ...  \n",
       "705                                          [removed]        1  \n",
       "706  Your comment has been removed because\\n\\n* **I...        0  \n",
       "707  They broke quarintine to tell others to not br...        1  \n",
       "708  If the protesters' goal is to clog up streets ...        1  \n",
       "709  They are blocking the protesters from blocking...        0  \n",
       "\n",
       "[710 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anti_lockdown_comments_df = pd.read_csv(folder_path + file1)\n",
    "anti_lockdown_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da3c3db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>commenter</th>\n",
       "      <th>comment</th>\n",
       "      <th>top_lvl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>g1hpwu</td>\n",
       "      <td>Johari82</td>\n",
       "      <td>Ending coronavirus lockdowns will be a dangero...</td>\n",
       "      <td>6967</td>\n",
       "      <td>1484</td>\n",
       "      <td>1.586912e+09</td>\n",
       "      <td>2020-04-15 08:46:34</td>\n",
       "      <td>Skooter_McGaven</td>\n",
       "      <td>I wish we had an understanding of where the ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>g1hpwu</td>\n",
       "      <td>Johari82</td>\n",
       "      <td>Ending coronavirus lockdowns will be a dangero...</td>\n",
       "      <td>6967</td>\n",
       "      <td>1484</td>\n",
       "      <td>1.586912e+09</td>\n",
       "      <td>2020-04-15 08:46:34</td>\n",
       "      <td>Richandler</td>\n",
       "      <td>&gt; Is it overly family spread?\\n\\nI believe the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>g1hpwu</td>\n",
       "      <td>Johari82</td>\n",
       "      <td>Ending coronavirus lockdowns will be a dangero...</td>\n",
       "      <td>6967</td>\n",
       "      <td>1484</td>\n",
       "      <td>1.586912e+09</td>\n",
       "      <td>2020-04-15 08:46:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This sounds horrible and I hate myself a littl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>g1hpwu</td>\n",
       "      <td>Johari82</td>\n",
       "      <td>Ending coronavirus lockdowns will be a dangero...</td>\n",
       "      <td>6967</td>\n",
       "      <td>1484</td>\n",
       "      <td>1.586912e+09</td>\n",
       "      <td>2020-04-15 08:46:34</td>\n",
       "      <td>lcbk</td>\n",
       "      <td>My husband and I are not yet confirmed to have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>g1hpwu</td>\n",
       "      <td>Johari82</td>\n",
       "      <td>Ending coronavirus lockdowns will be a dangero...</td>\n",
       "      <td>6967</td>\n",
       "      <td>1484</td>\n",
       "      <td>1.586912e+09</td>\n",
       "      <td>2020-04-15 08:46:34</td>\n",
       "      <td>ZombiGrn</td>\n",
       "      <td>In my neighborhood people started throwing par...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>0</td>\n",
       "      <td>gr29as</td>\n",
       "      <td>frequenttimetraveler</td>\n",
       "      <td>Strict Physical Distancing May Be More Efficie...</td>\n",
       "      <td>909</td>\n",
       "      <td>352</td>\n",
       "      <td>1.590517e+09</td>\n",
       "      <td>2020-05-27 02:10:44</td>\n",
       "      <td>stillobsessed</td>\n",
       "      <td>A surprise lockdown period of 30 days wouldn't...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>0</td>\n",
       "      <td>gr29as</td>\n",
       "      <td>frequenttimetraveler</td>\n",
       "      <td>Strict Physical Distancing May Be More Efficie...</td>\n",
       "      <td>909</td>\n",
       "      <td>352</td>\n",
       "      <td>1.590517e+09</td>\n",
       "      <td>2020-05-27 02:10:44</td>\n",
       "      <td>thisrockismyboone</td>\n",
       "      <td>Society would cease to function without infras...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0</td>\n",
       "      <td>gr29as</td>\n",
       "      <td>frequenttimetraveler</td>\n",
       "      <td>Strict Physical Distancing May Be More Efficie...</td>\n",
       "      <td>909</td>\n",
       "      <td>352</td>\n",
       "      <td>1.590517e+09</td>\n",
       "      <td>2020-05-27 02:10:44</td>\n",
       "      <td>SamH123</td>\n",
       "      <td>\\- this depends a bit on what stage of spread ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0</td>\n",
       "      <td>gr29as</td>\n",
       "      <td>frequenttimetraveler</td>\n",
       "      <td>Strict Physical Distancing May Be More Efficie...</td>\n",
       "      <td>909</td>\n",
       "      <td>352</td>\n",
       "      <td>1.590517e+09</td>\n",
       "      <td>2020-05-27 02:10:44</td>\n",
       "      <td>reini_urban</td>\n",
       "      <td>Strict physical distance may be statistically ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>0</td>\n",
       "      <td>gr29as</td>\n",
       "      <td>frequenttimetraveler</td>\n",
       "      <td>Strict Physical Distancing May Be More Efficie...</td>\n",
       "      <td>909</td>\n",
       "      <td>352</td>\n",
       "      <td>1.590517e+09</td>\n",
       "      <td>2020-05-27 02:10:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0      id                author  \\\n",
       "0             0  g1hpwu              Johari82   \n",
       "1             0  g1hpwu              Johari82   \n",
       "2             0  g1hpwu              Johari82   \n",
       "3             0  g1hpwu              Johari82   \n",
       "4             0  g1hpwu              Johari82   \n",
       "..          ...     ...                   ...   \n",
       "455           0  gr29as  frequenttimetraveler   \n",
       "456           0  gr29as  frequenttimetraveler   \n",
       "457           0  gr29as  frequenttimetraveler   \n",
       "458           0  gr29as  frequenttimetraveler   \n",
       "459           0  gr29as  frequenttimetraveler   \n",
       "\n",
       "                                                 title  score  comms_num  \\\n",
       "0    Ending coronavirus lockdowns will be a dangero...   6967       1484   \n",
       "1    Ending coronavirus lockdowns will be a dangero...   6967       1484   \n",
       "2    Ending coronavirus lockdowns will be a dangero...   6967       1484   \n",
       "3    Ending coronavirus lockdowns will be a dangero...   6967       1484   \n",
       "4    Ending coronavirus lockdowns will be a dangero...   6967       1484   \n",
       "..                                                 ...    ...        ...   \n",
       "455  Strict Physical Distancing May Be More Efficie...    909        352   \n",
       "456  Strict Physical Distancing May Be More Efficie...    909        352   \n",
       "457  Strict Physical Distancing May Be More Efficie...    909        352   \n",
       "458  Strict Physical Distancing May Be More Efficie...    909        352   \n",
       "459  Strict Physical Distancing May Be More Efficie...    909        352   \n",
       "\n",
       "          created            timestamp          commenter  \\\n",
       "0    1.586912e+09  2020-04-15 08:46:34    Skooter_McGaven   \n",
       "1    1.586912e+09  2020-04-15 08:46:34         Richandler   \n",
       "2    1.586912e+09  2020-04-15 08:46:34                NaN   \n",
       "3    1.586912e+09  2020-04-15 08:46:34               lcbk   \n",
       "4    1.586912e+09  2020-04-15 08:46:34           ZombiGrn   \n",
       "..            ...                  ...                ...   \n",
       "455  1.590517e+09  2020-05-27 02:10:44      stillobsessed   \n",
       "456  1.590517e+09  2020-05-27 02:10:44  thisrockismyboone   \n",
       "457  1.590517e+09  2020-05-27 02:10:44            SamH123   \n",
       "458  1.590517e+09  2020-05-27 02:10:44        reini_urban   \n",
       "459  1.590517e+09  2020-05-27 02:10:44                NaN   \n",
       "\n",
       "                                               comment  top_lvl  \n",
       "0    I wish we had an understanding of where the ma...        1  \n",
       "1    > Is it overly family spread?\\n\\nI believe the...        0  \n",
       "2    This sounds horrible and I hate myself a littl...        0  \n",
       "3    My husband and I are not yet confirmed to have...        0  \n",
       "4    In my neighborhood people started throwing par...        0  \n",
       "..                                                 ...      ...  \n",
       "455  A surprise lockdown period of 30 days wouldn't...        0  \n",
       "456  Society would cease to function without infras...        0  \n",
       "457  \\- this depends a bit on what stage of spread ...        0  \n",
       "458  Strict physical distance may be statistically ...        1  \n",
       "459                                          [removed]        1  \n",
       "\n",
       "[460 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_lockdown_comments_df = pd.read_csv(folder_path + file2)\n",
    "corona_lockdown_comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39185d7",
   "metadata": {},
   "source": [
    "<h3>Cleaning Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa0112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with removed comments\n",
    "anti_lockdown_comments_df = anti_lockdown_comments_df[anti_lockdown_comments_df.comment != \"[removed]\"]\n",
    "corona_lockdown_comments_df = corona_lockdown_comments_df[corona_lockdown_comments_df.comment != \"[removed]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8b23db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095\n",
      "People like this everywhere in the world.  Difference is, are there enough to gain political power?\n",
      "Well, there's certainly enough of them to cause a massive surge in COVID-19 cases and deaths, and to take some of us with them.\n"
     ]
    }
   ],
   "source": [
    "list1 = anti_lockdown_comments_df[\"comment\"].tolist()\n",
    "list2 = corona_lockdown_comments_df[\"comment\"].tolist()\n",
    "comments = list1 + list2\n",
    "print(len(comments))\n",
    "print(comments[0])\n",
    "print(comments[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4205e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text):\n",
    "    remove_chars = '[0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "    return re.sub(remove_chars, ' ', text)\n",
    "\n",
    "def remove_digit_strings(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    result_text = remove_links(text)\n",
    "    result_text = remove_digit_strings(result_text)\n",
    "    result_text = remove_special_chars(result_text)\n",
    "    result_text = result_text.lower()\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce478f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(comments)):\n",
    "    comments[i] = clean_text(comments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75628050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people like this everywhere in the world   difference is  are there enough to gain political power \n",
      "well  there s certainly enough of them to cause a massive surge in covid  cases and deaths  and to take some of us with them \n"
     ]
    }
   ],
   "source": [
    "print(comments[0])\n",
    "print(comments[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953e04f",
   "metadata": {},
   "source": [
    "<h3>Tokenizing Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "902fdb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "exclude_words = stop_words\n",
    "\n",
    "#exclude common words \n",
    "exclude_words_extra = [\"covid\",\"lockdown\", \"pandemic\",\"get\",\"go\",\"let\",\"im\",\"ive\",\"would\",\"one\",\"also\",\"to\",\"say\",\"day\",\"well\",\"month\",\"thing\"]\n",
    "\n",
    "exclude_words.extend(exclude_words_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b384d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4986b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people', 'like', 'this', 'everywhere', 'in', 'the', 'world', 'difference', 'is', 'are', 'there', 'enough', 'to', 'gain', 'political', 'power']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b07d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in exclude_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ce43a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['people', 'everywhere', 'world', 'difference', 'enough', 'gain', 'political', 'power']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c64fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('difference', 1),\n",
       "  ('enough', 1),\n",
       "  ('everywhere', 1),\n",
       "  ('gain', 1),\n",
       "  ('people', 1),\n",
       "  ('political', 1),\n",
       "  ('power', 1),\n",
       "  ('world', 1)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d2b17",
   "metadata": {},
   "source": [
    "<h3>LDA Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca2811dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "# supporting function 2\n",
    "def compute_perplexity_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    perplexity_score = lda_model.log_perplexity(corpus_sets[i])\n",
    "    \n",
    "    return perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670bd8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|▎                                                                             | 1/271 [02:45<12:24:36, 165.47s/it]\u001b[A\n",
      "\n",
      "  0%|▎                                                                              | 1/271 [01:46<7:57:28, 106.10s/it]\u001b[A\n",
      "  1%|▌                                                                              | 2/271 [03:32<7:55:54, 106.15s/it]\u001b[A\n",
      "  1%|▊                                                                              | 3/271 [05:16<7:49:50, 105.19s/it]\u001b[A\n",
      "  1%|█▏                                                                             | 4/271 [06:58<7:43:15, 104.10s/it]\u001b[A\n",
      "  2%|█▍                                                                             | 5/271 [08:43<7:42:05, 104.23s/it]\u001b[A\n",
      "  2%|█▋                                                                             | 6/271 [10:34<7:50:51, 106.61s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               # gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "corpus_title = ['100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': [],\n",
    "                 'Perplexity': []\n",
    "                }\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=271)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    p = compute_perplexity_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    model_results['Perplexity'].append(p)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('./lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4526aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a34e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = lda_model\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bde749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, atopic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec5598",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summaries = []\n",
    "print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "for i in range(4):\n",
    "    print('Topic '+str(i)+' |---------------------\\n')\n",
    "    tmp = explore_topic(lda_model,topic_number=i, topn=10, output=True )\n",
    "#     print tmp[:5]\n",
    "    topic_summaries += [tmp[:5]]\n",
    "    print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
