{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb03b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../Twitter_Data/\"\n",
    "cn_folder_path = \"../Twitter_Data/Twitter_Data_Chinese/\"\n",
    "\n",
    "visuals_output_path = \"./visuals/\"\n",
    "output_path = \"./output/\"\n",
    "\n",
    "file1 = \"Twitter_Covid-19_Lockdown_5000.csv\"\n",
    "file2 = \"Twitter_Jan_Mar_5000.csv\"\n",
    "file3 = \"Twitter_Mar_5000.csv\"\n",
    "file4 = \"Twitter_May_Nov_5000.csv\"\n",
    "\n",
    "cn_file1 = \"en.Twitter_Covid-19_Lockdown_5000_chinese.csv\"\n",
    "cn_file2 = \"en.Twitter_Jan_Apr_2020_5000_chinese.csv\"\n",
    "cn_file3 = \"en.Twitter_May_June_2022_5000_chinese.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "876aee79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\lindy\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.22.4)\n",
      "Requirement already satisfied: funcy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.18)\n",
      "Requirement already satisfied: numexpr in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: pandas>=1.3.4 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.7.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (58.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: gensim in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.3.4->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2980df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lindy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords') #download if don't have yet\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer # tokenize words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6051e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in files\n",
    "df1 = pd.read_csv(folder_path + file1)\n",
    "df2 = pd.read_csv(folder_path + file2)\n",
    "df3 = pd.read_csv(folder_path + file3)\n",
    "df4 = pd.read_csv(folder_path + file3)\n",
    "\n",
    "cn_df1 = pd.read_csv(cn_folder_path + cn_file1)\n",
    "cn_df2 = pd.read_csv(cn_folder_path + cn_file2)\n",
    "cn_df3 = pd.read_csv(cn_folder_path + cn_file3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a0a3f",
   "metadata": {},
   "source": [
    "<h1>Cleaning Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f317c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns mentioning Bheed trailer\n",
    "df1 = df1[df1[\"text\"].str.contains(\"Bheed\")==False]\n",
    "df2 = df2[df2[\"Text\"].str.contains(\"Bheed\")==False]\n",
    "df3 = df3[df3[\"Text\"].str.contains(\"Bheed\")==False]\n",
    "df4 = df4[df4[\"Text\"].str.contains(\"Bheed\")==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad15245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = df1[\"text\"].tolist()\n",
    "list2 = df2[\"Text\"].tolist()\n",
    "list3 = df3[\"Text\"].tolist()\n",
    "list4 = df4[\"Text\"].tolist()\n",
    "\n",
    "cn_list1 = cn_df1[\"text\"].tolist()\n",
    "cn_list2 = cn_df2[\"text\"].tolist()\n",
    "cn_list3 = cn_df3[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6c234f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_user_mentions(text):\n",
    "    return re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_digit_strings(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    remove_chars = '[0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "    return re.sub(remove_chars, ' ', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    result_text = text\n",
    "    result_text = remove_user_mentions(result_text)\n",
    "    result_text = remove_links(result_text)\n",
    "    result_text = remove_digit_strings(result_text)\n",
    "    result_text = remove_special_chars(result_text)\n",
    "    result_text = result_text.lower()\n",
    "    return result_text\n",
    "\n",
    "def remove_cn_chars(text):\n",
    "    result_text = re.sub(r'([\\u4e00-\\u9fff]+', '', text)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f7a8323",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "missing ), unterminated subpattern at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26500/1110149686.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_list2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mcn_list2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_cn_chars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_list2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mcn_list2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_list2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26500/1550242137.py\u001b[0m in \u001b[0;36mremove_cn_chars\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_cn_chars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mresult_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'([\\u4e00-\\u9fff]+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m         \u001b[1;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0m\u001b[0;32m    444\u001b[0m                            not nested and not items))\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_verbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnested\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\")\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m                 raise source.error(\"missing ), unterminated subpattern\",\n\u001b[0m\u001b[0;32m    837\u001b[0m                                    source.tell() - start)\n\u001b[0;32m    838\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: missing ), unterminated subpattern at position 0"
     ]
    }
   ],
   "source": [
    "for i in range(len(list1)):\n",
    "    list1[i] = clean_text(list1[i])\n",
    "\n",
    "for i in range(len(list2)):\n",
    "    list2[i] = clean_text(list2[i])\n",
    "\n",
    "for i in range(len(list3)):\n",
    "    list3[i] = clean_text(list3[i])\n",
    "    \n",
    "for i in range(len(list4)):\n",
    "    list4[i] = clean_text(list4[i])\n",
    "    \n",
    "for i in range(len(cn_list1)):\n",
    "    cn_list1[i] = remove_cn_chars(cn_list1[i])\n",
    "    cn_list1[i] = clean_text(cn_list1[i])\n",
    "\n",
    "for i in range(len(cn_list2)):\n",
    "    cn_list2[i] = remove_cn_chars(cn_list2[i])\n",
    "    cn_list2[i] = clean_text(cn_list2[i])\n",
    "\n",
    "for i in range(len(cn_list3)):\n",
    "    cn_list3[i] = remove_cn_chars(cn_list3[i])\n",
    "    cn_list3[i] = clean_text(cn_list3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"cleaned_text\"] = list1\n",
    "df2[\"cleaned_text\"] = list2\n",
    "df3[\"cleaned_text\"] = list3\n",
    "df4[\"cleaned_text\"] = list4\n",
    "\n",
    "cn_df1[\"cleaned_text\"] = cn_list1\n",
    "cn_df2[\"cleaned_text\"] = cn_list2\n",
    "cn_df3[\"cleaned_text\"] = cn_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of period of tweets to information associated with tweet\n",
    "\n",
    "tweets = {}\n",
    "tweets[\"covid_lockdown\"][\"text\"] = df1[\"cleaned_text\"].tolist()\n",
    "tweets[\"jan_mar\"][\"text\"] = df2[\"cleaned_text\"].tolist()\n",
    "tweets[\"mar\"][\"text\"] = df3[\"cleaned_text\"].tolist()\n",
    "tweets[\"may_nov\"][\"text\"] = df4[\"cleaned_text\"].tolist()\n",
    "\n",
    "tweets[\"cn_covid_lockdown\"][\"text\"] = cn_df1[\"cleaned_text\"].tolist()\n",
    "tweets[\"cn_jan_apr_2020\"][\"text\"] = cn_df2[\"cleaned_text\"].tolist()\n",
    "tweets[\"cn_may_june_2022\"][\"text\"] = cn_df3[\"cleaned_text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1fd069",
   "metadata": {},
   "source": [
    "<h1>Tokenizing Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ccc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get custom stopwords\n",
    "content = []\n",
    "f = open(\"./stopwords.txt\", encoding = 'utf-8')\n",
    "# perform file operations\n",
    "for line in f:\n",
    "    content.append(line)\n",
    "f.close()\n",
    "\n",
    "custom_stopwords = []\n",
    "for line in content:\n",
    "    wordlist = line.split(\",\")\n",
    "    for word in wordlist:\n",
    "        custom_stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "exclude_words = stop_words\n",
    "\n",
    "exclude_words_extra = [\"RT\",\"still\",\"covid\",\"coronavirus\",\"lockdown\",\"lockdo\",\"pandemic\",\"let\",\"get\",\"ago\",\"go\",\"im\",\"ive\",\"would\",\"one\",\"also\",\"to\",\"tag\"]\n",
    "\n",
    "# Exclude custom stopwords\n",
    "exclude_words.extend(custom_stopwords)\n",
    "exclude_words.extend(exclude_words_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f555a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "for period in tweets.keys():\n",
    "    tweets[period][\"data_words\"] = list(sent_to_words(tweets[period][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe84e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_trigram_models(period):\n",
    "    data_words = tweets[period][\"data_words\"]\n",
    "    \n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "    \n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    tweets[period][\"bigram\"] = bigram\n",
    "    tweets[period][\"trigram\"] = trigram\n",
    "    tweets[period][\"bigram_mod\"] = bigram_mod\n",
    "    tweets[period][\"trigram_mod\"] = trigram_mod\n",
    "\n",
    "for period in tweets.keys():\n",
    "    bigram_trigram_models(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb3d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in exclude_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts, bigram_mod, trigram_mod):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined(period):\n",
    "    data_words = tweets[period][\"data_words\"]\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "    data_words_nostops = data_words\n",
    "    tweets[period][\"data_words_nostops\"] = data_words_nostops \n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops, tweets[period][\"bigram_mod\"])\n",
    "    tweets[period][\"data_words_bigrams\"] = data_words_bigrams\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    tweets[period][\"data_lemmatized\"] = data_lemmatized\n",
    "\n",
    "for period in tweets.keys():\n",
    "    combined(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1960d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets[list(tweets.keys())[0]].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(period):\n",
    "    data_lemmatized = tweets[period][\"data_lemmatized\"]\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "    tweets[period][\"id2word\"] = id2word\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "    tweets[period][\"texts\"] = texts\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    tweets[period][\"corpus\"] = corpus\n",
    "\n",
    "    # Human readable format of corpus (term-frequency)\n",
    "    tweets[period][\"corpus_readable\"] = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "for period in tweets.keys():\n",
    "    tokenize(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"corpus_readable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf76de",
   "metadata": {},
   "source": [
    "<h1>LDA Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038439ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "# supporting function 2\n",
    "def compute_perplexity_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    perplexity_score = lda_model.log_perplexity(corpus_sets[i])\n",
    "    \n",
    "    return perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(period):\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    id2word = tweets[period][\"id2word\"]\n",
    "\n",
    "    # Build LDA model\n",
    "    num_topics = 4\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=num_topics, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "    tweets[period][\"lda_model\"] = lda_model\n",
    "\n",
    "for period in tweets.keys():\n",
    "    lda_model(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"lda_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c257547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_in_topics(period):\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "\n",
    "    # Print the Keyword in the 10 topics\n",
    "    pprint(lda_model.print_topics())\n",
    "    doc_lda = lda_model[corpus]\n",
    "    tweets[period][\"doc_lda\"] = doc_lda\n",
    "\n",
    "for period in tweets.keys():\n",
    "    keyword_in_topics(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1181e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"doc_lda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc97fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_coherence(period):\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    data_lemmatized = tweets[period][\"data_lemmatized\"]\n",
    "    id2word = tweets[period][\"id2word\"]\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "\n",
    "    # Compute Perplexity\n",
    "    tweets[period][\"perplexity\"] = lda_model.log_perplexity(corpus)\n",
    "    # print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "    tweets[period][\"coherence_model_lda\"] = coherence_model_lda\n",
    "    tweets[period][\"coherence_lda\"] = coherence_lda\n",
    "\n",
    "    # print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "for period in tweets.keys():\n",
    "    perplexity_coherence(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"perplexity\"])\n",
    "print(tweets[list(tweets.keys())[index]][\"coherence_lda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdba215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_model(period): \n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "\n",
    "    optimal_model = lda_model\n",
    "    model_topics = optimal_model.show_topics(formatted=False)\n",
    "\n",
    "    tweets[period][\"model_topics\"] = model_topics\n",
    "\n",
    "for period in tweets.keys():\n",
    "    optimal_model(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8028a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"model_topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, atopic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9528751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic_by_title(period):\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "    topic_summaries = []\n",
    "\n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "    print(period)\n",
    "    for i in range(4):\n",
    "        print('Topic '+str(i)+' |---------------------\\n')\n",
    "        tmp = explore_topic(lda_model,topic_number=i, topn=10, output=True )\n",
    "        #     print tmp[:5]\n",
    "        topic_summaries += [tmp[:5]]\n",
    "        # print\n",
    "\n",
    "    tweets[period][\"topic_summaries\"] = topic_summaries\n",
    "\n",
    "for period in tweets.keys():\n",
    "    explore_topic_by_title(period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e8e40",
   "metadata": {},
   "source": [
    "<h1>Data Output - Dominant Topic</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "for period in tweets.keys():\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    texts = tweets[period][\"texts\"]\n",
    "    \n",
    "    df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, texts)\n",
    "    \n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    \n",
    "    tweets[period][\"df_dominant_topic\"] = df_dominant_topic\n",
    "    \n",
    "    df_dominant_topic.to_csv(output_path + period + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ecad4c",
   "metadata": {},
   "source": [
    "<h1>Data Visualization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53774098",
   "metadata": {},
   "source": [
    "<h3>PyLDAVis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "# Visualize the topics\n",
    "for period in tweets.keys():\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    id2word = tweets[period][\"id2word\"]\n",
    "\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    \n",
    "    file_path = visuals_output_path + period + '.html'\n",
    "    pyLDAvis.save_html(LDAvis_prepared, file_path)\n",
    "    \n",
    "    with open(file_path, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write(\"<h1>\" + period + \"</h1>\" + '\\n' + content)\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
