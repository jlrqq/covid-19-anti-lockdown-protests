{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "83dbc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../Twitter_Data/\"\n",
    "cn_folder_path = \"../Twitter_Data/Twitter_Data_Chinese/\"\n",
    "\n",
    "visuals_output_path = \"./visuals/\"\n",
    "output_path = \"./output/\"\n",
    "\n",
    "file1 = \"Twitter_Covid-19_Lockdown_5000.csv\"\n",
    "file2 = \"Twitter_Jan_Mar_5000.csv\"\n",
    "file3 = \"Twitter_Mar_5000.csv\"\n",
    "file4 = \"Twitter_May_Nov_5000.csv\"\n",
    "\n",
    "cn_file1 = \"en.Twitter_Covid-19_Lockdown_5000_chinese.csv\"\n",
    "cn_file2 = \"en.Twitter_Jan_Apr_2020_5000_chinese.csv\"\n",
    "cn_file3 = \"en.Twitter_May_June_2022_5000_chinese.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "23957281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\lindy\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.22.4)\n",
      "Requirement already satisfied: pandas>=1.3.4 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.4)\n",
      "Requirement already satisfied: numexpr in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (58.0.4)\n",
      "Requirement already satisfied: funcy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.18)\n",
      "Requirement already satisfied: scipy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.3.4->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b145bc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lindy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords') #download if don't have yet\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer # tokenize words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "abc653eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in files\n",
    "df1 = pd.read_csv(folder_path + file1)\n",
    "df2 = pd.read_csv(folder_path + file2)\n",
    "df3 = pd.read_csv(folder_path + file3)\n",
    "df4 = pd.read_csv(folder_path + file3)\n",
    "\n",
    "cn_df1 = pd.read_csv(cn_folder_path + cn_file1)\n",
    "cn_df2 = pd.read_csv(cn_folder_path + cn_file2)\n",
    "cn_df3 = pd.read_csv(cn_folder_path + cn_file3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05315a87",
   "metadata": {},
   "source": [
    "<h1>Cleaning Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "09624baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns mentioning Bheed trailer\n",
    "df1 = df1[df1[\"text\"].str.contains(\"Bheed\")==False]\n",
    "df2 = df2[df2[\"Text\"].str.contains(\"Bheed\")==False]\n",
    "df3 = df3[df3[\"Text\"].str.contains(\"Bheed\")==False]\n",
    "df4 = df4[df4[\"Text\"].str.contains(\"Bheed\")==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a7d03793",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = df1[\"text\"].tolist()\n",
    "list2 = df2[\"Text\"].tolist()\n",
    "list3 = df3[\"Text\"].tolist()\n",
    "list4 = df4[\"Text\"].tolist()\n",
    "\n",
    "cn_list1 = cn_df1[\"text\"].tolist()\n",
    "cn_list2 = cn_df2[\"text\"].tolist()\n",
    "cn_list3 = cn_df3[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "11ee88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_user_mentions(text):\n",
    "    return re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_digit_strings(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    remove_chars = '[0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "    return re.sub(remove_chars, ' ', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    result_text = text\n",
    "    result_text = remove_user_mentions(result_text)\n",
    "    result_text = remove_links(result_text)\n",
    "    result_text = remove_digit_strings(result_text)\n",
    "    result_text = remove_special_chars(result_text)\n",
    "    result_text = result_text.lower()\n",
    "    return result_text\n",
    "\n",
    "def remove_cn_chars(text):\n",
    "    result_text = re.sub(r'([\\u4e00-\\u9fff]+', '', text)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3051b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list1)):\n",
    "    list1[i] = clean_text(list1[i])\n",
    "\n",
    "for i in range(len(list2)):\n",
    "    list2[i] = clean_text(list2[i])\n",
    "\n",
    "for i in range(len(list3)):\n",
    "    list3[i] = clean_text(list3[i])\n",
    "    \n",
    "for i in range(len(list4)):\n",
    "    list4[i] = clean_text(list4[i])\n",
    "    \n",
    "for i in range(len(cn_list1)):\n",
    "#     cn_list1[i] = remove_cn_chars(cn_list1[i])\n",
    "    cn_list1[i] = clean_text(cn_list1[i])\n",
    "\n",
    "for i in range(len(cn_list2)):\n",
    "#     cn_list2[i] = remove_cn_chars(cn_list2[i])\n",
    "    cn_list2[i] = clean_text(str(cn_list2[i]))\n",
    "\n",
    "for i in range(len(cn_list3)):\n",
    "#      cn_list3[i] = remove_cn_chars(cn_list3[i])\n",
    "    cn_list3[i] = clean_text(str(cn_list3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c428790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"cleaned_text\"] = list1\n",
    "df2[\"cleaned_text\"] = list2\n",
    "df3[\"cleaned_text\"] = list3\n",
    "df4[\"cleaned_text\"] = list4\n",
    "\n",
    "cn_df1[\"cleaned_text\"] = cn_list1\n",
    "cn_df2[\"cleaned_text\"] = cn_list2\n",
    "cn_df3[\"cleaned_text\"] = cn_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8e195513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of period of tweets to information associated with tweet\n",
    "\n",
    "tweets = {}\n",
    "keys = [\"covid_lockdown\", \"jan_mar\", \"mar\", \"may_nov\", \"cn_covid_lockdown\", \"cn_jan_apr_2020\", \"cn_may_june_2022\"]\n",
    "for key in keys:\n",
    "    tweets[key] = {}\n",
    "\n",
    "tweets[\"covid_lockdown\"][\"text\"] = df1[\"cleaned_text\"].tolist()\n",
    "tweets[\"jan_mar\"][\"text\"] = df2[\"cleaned_text\"].tolist()\n",
    "tweets[\"mar\"][\"text\"] = df3[\"cleaned_text\"].tolist()\n",
    "tweets[\"may_nov\"][\"text\"] = df4[\"cleaned_text\"].tolist()\n",
    "\n",
    "tweets[\"cn_covid_lockdown\"][\"text\"] = cn_df1[\"cleaned_text\"].tolist()\n",
    "tweets[\"cn_jan_apr_2020\"][\"text\"] = cn_df2[\"cleaned_text\"].tolist()\n",
    "tweets[\"cn_may_june_2022\"][\"text\"] = cn_df3[\"cleaned_text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127edb4",
   "metadata": {},
   "source": [
    "<h1>Tokenizing Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "015218ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get custom stopwords\n",
    "content = []\n",
    "f = open(\"./stopwords.txt\", encoding = 'utf-8')\n",
    "# perform file operations\n",
    "for line in f:\n",
    "    content.append(line)\n",
    "f.close()\n",
    "\n",
    "custom_stopwords = []\n",
    "for line in content:\n",
    "    wordlist = line.split(\",\")\n",
    "    for word in wordlist:\n",
    "        custom_stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5fb4491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "exclude_words = stop_words\n",
    "\n",
    "exclude_words_extra = [\"RT\",\"still\",\"covid\",\"coronavirus\",\"lockdown\",\"lockdo\",\"pandemic\",\"epidemic\",\"let\",\"get\",\"ago\",\"go\",\"im\",\"ive\",\"would\",\"one\",\"also\",\"to\",\"tag\"]\n",
    "\n",
    "# Exclude custom stopwords\n",
    "exclude_words.extend(custom_stopwords)\n",
    "exclude_words.extend(exclude_words_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "645a7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "for period in tweets.keys():\n",
    "    tweets[period][\"data_words\"] = list(sent_to_words(tweets[period][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "28ce038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_trigram_models(period):\n",
    "    data_words = tweets[period][\"data_words\"]\n",
    "    \n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "    \n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    tweets[period][\"bigram\"] = bigram\n",
    "    tweets[period][\"trigram\"] = trigram\n",
    "    tweets[period][\"bigram_mod\"] = bigram_mod\n",
    "    tweets[period][\"trigram_mod\"] = trigram_mod\n",
    "\n",
    "for period in tweets.keys():\n",
    "    bigram_trigram_models(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0fb35563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in exclude_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts, bigram_mod, trigram_mod):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d11d7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined(period):\n",
    "    data_words = tweets[period][\"data_words\"]\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "    tweets[period][\"data_words_nostops\"] = data_words_nostops \n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops, tweets[period][\"bigram_mod\"])\n",
    "    tweets[period][\"data_words_bigrams\"] = data_words_bigrams\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    tweets[period][\"data_lemmatized\"] = data_lemmatized\n",
    "\n",
    "for period in tweets.keys():\n",
    "    combined(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ad9a2184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for period in tweets.keys():\n",
    "#     data_lemmatized = tweets[period][\"data_lemmatized\"]\n",
    "#     for arr in data_lemmatized:\n",
    "#         for word in arr:\n",
    "#             if word in exclude_words:\n",
    "#                 arr.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "54af5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(period):\n",
    "    data_lemmatized = tweets[period][\"data_lemmatized\"]\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "    tweets[period][\"id2word\"] = id2word\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "    tweets[period][\"texts\"] = texts\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    tweets[period][\"corpus\"] = corpus\n",
    "\n",
    "    # Human readable format of corpus (term-frequency)\n",
    "    tweets[period][\"corpus_readable\"] = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "for period in tweets.keys():\n",
    "    tokenize(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e8c0afaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('home', 1), ('place', 1)]]\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"corpus_readable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2800f0c",
   "metadata": {},
   "source": [
    "<h1>LDA Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b2314087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "# supporting function 2\n",
    "def compute_perplexity_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    perplexity_score = lda_model.log_perplexity(corpus_sets[i])\n",
    "    \n",
    "    return perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6cb92fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(period):\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    id2word = tweets[period][\"id2word\"]\n",
    "\n",
    "    # Build LDA model\n",
    "    num_topics = 4\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=num_topics, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "    tweets[period][\"lda_model\"] = lda_model\n",
    "\n",
    "for period in tweets.keys():\n",
    "    lda_model(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5e7fa97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel<num_terms=2890, num_topics=4, decay=0.5, chunksize=100>\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"lda_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "aa2de5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.067*\"year\" + 0.035*\"first\" + 0.020*\"today\" + 0.019*\"government\" + '\n",
      "  '0.016*\"come\" + 0.015*\"issue\" + 0.015*\"third\" + 0.012*\"see\" + 0.012*\"series\" '\n",
      "  '+ 0.012*\"due\"'),\n",
      " (1,\n",
      "  '0.025*\"support\" + 0.018*\"happen\" + 0.017*\"mental\" + 0.016*\"former\" + '\n",
      "  '0.016*\"boris_johnson\" + 0.015*\"hlth\" + 0.015*\"collect\" + '\n",
      "  '0.014*\"expenditure_data\" + 0.013*\"party\" + 0.013*\"truth\"'),\n",
      " (2,\n",
      "  '0.022*\"make\" + 0.021*\"impact\" + 0.017*\"post\" + 0.017*\"population\" + '\n",
      "  '0.016*\"mask\" + 0.016*\"wear\" + 0.015*\"inequality\" + 0.014*\"people\" + '\n",
      "  '0.012*\"world\" + 0.011*\"lockdown\"'),\n",
      " (3,\n",
      "  '0.055*\"first\" + 0.043*\"year\" + 0.041*\"day\" + 0.035*\"anniversary\" + '\n",
      "  '0.030*\"today\" + 0.025*\"die\" + 0.025*\"national\" + 0.024*\"mark\" + '\n",
      "  '0.020*\"time\" + 0.019*\"remember\"')]\n",
      "[(0,\n",
      "  '0.015*\"week\" + 0.011*\"news\" + 0.011*\"life\" + 0.010*\"post\" + 0.010*\"call\" + '\n",
      "  '0.009*\"second\" + 0.009*\"force\" + 0.009*\"disease\" + 0.008*\"video\" + '\n",
      "  '0.008*\"mask\"'),\n",
      " (1,\n",
      "  '0.049*\"people\" + 0.042*\"day\" + 0.023*\"case\" + 0.020*\"city\" + 0.017*\"town\" + '\n",
      "  '0.016*\"country\" + 0.015*\"outbreak\" + 0.015*\"resident\" + 0.015*\"new\" + '\n",
      "  '0.013*\"say\"'),\n",
      " (2,\n",
      "  '0.023*\"ncov\" + 0.022*\"put\" + 0.019*\"spread\" + 0.017*\"leave\" + 0.016*\"case\" '\n",
      "  '+ 0.014*\"first\" + 0.013*\"quarantine\" + 0.013*\"population\" + 0.012*\"virus\" + '\n",
      "  '0.012*\"due\"'),\n",
      " (3,\n",
      "  '0.019*\"report\" + 0.018*\"quarantine\" + 0.016*\"measure\" + 0.015*\"take\" + '\n",
      "  '0.012*\"apartment_amidst\" + 0.011*\"amp\" + 0.011*\"novel\" + 0.010*\"city\" + '\n",
      "  '0.010*\"base\" + 0.010*\"outbreak\"')]\n",
      "[(0,\n",
      "  '0.052*\"first\" + 0.047*\"indian\" + 0.022*\"time\" + 0.016*\"quarantine\" + '\n",
      "  '0.013*\"government\" + 0.011*\"coronaupdate\" + 0.011*\"country\" + 0.011*\"good\" '\n",
      "  '+ 0.010*\"food\" + 0.010*\"world\"'),\n",
      " (1,\n",
      "  '0.047*\"tune\" + 0.046*\"drive\" + 0.037*\"exclusive\" + 0.037*\"skoda\" + '\n",
      "  '0.037*\"soil\" + 0.019*\"home\" + 0.014*\"help\" + 0.012*\"stay\" + 0.008*\"fight\" + '\n",
      "  '0.008*\"live\"'),\n",
      " (2,\n",
      "  '0.045*\"right\" + 0.041*\"catch\" + 0.028*\"people\" + 0.014*\"need\" + '\n",
      "  '0.013*\"case\" + 0.012*\"go\" + 0.012*\"due\" + 0.010*\"say\" + 0.010*\"amp\" + '\n",
      "  '0.009*\"take\"'),\n",
      " (3,\n",
      "  '0.044*\"day\" + 0.020*\"stayhome\" + 0.013*\"total\" + 0.012*\"state\" + '\n",
      "  '0.011*\"virus\" + 0.010*\"spread\" + 0.010*\"come\" + 0.009*\"stop\" + 0.009*\"make\" '\n",
      "  '+ 0.008*\"period\"')]\n",
      "[(0,\n",
      "  '0.052*\"first\" + 0.047*\"indian\" + 0.022*\"time\" + 0.016*\"quarantine\" + '\n",
      "  '0.013*\"government\" + 0.011*\"coronaupdate\" + 0.011*\"country\" + 0.011*\"good\" '\n",
      "  '+ 0.010*\"food\" + 0.010*\"world\"'),\n",
      " (1,\n",
      "  '0.047*\"tune\" + 0.046*\"drive\" + 0.037*\"exclusive\" + 0.037*\"skoda\" + '\n",
      "  '0.037*\"soil\" + 0.019*\"home\" + 0.014*\"help\" + 0.012*\"stay\" + 0.008*\"fight\" + '\n",
      "  '0.008*\"live\"'),\n",
      " (2,\n",
      "  '0.045*\"right\" + 0.041*\"catch\" + 0.028*\"people\" + 0.014*\"need\" + '\n",
      "  '0.013*\"case\" + 0.012*\"go\" + 0.012*\"due\" + 0.010*\"say\" + 0.010*\"amp\" + '\n",
      "  '0.009*\"take\"'),\n",
      " (3,\n",
      "  '0.044*\"day\" + 0.020*\"stayhome\" + 0.013*\"total\" + 0.012*\"state\" + '\n",
      "  '0.011*\"virus\" + 0.010*\"spread\" + 0.010*\"come\" + 0.009*\"stop\" + 0.009*\"make\" '\n",
      "  '+ 0.008*\"period\"')]\n",
      "[(0,\n",
      "  '0.030*\"closure\" + 0.025*\"people\" + 0.021*\"city\" + 0.018*\"first\" + '\n",
      "  '0.017*\"year\" + 0.013*\"time\" + 0.011*\"even\" + 0.011*\"say\" + 0.010*\"die\" + '\n",
      "  '0.009*\"leave\"'),\n",
      " (1,\n",
      "  '0.035*\"city\" + 0.035*\"year\" + 0.027*\"big\" + 0.025*\"business\" + '\n",
      "  '0.022*\"closure\" + 0.018*\"even\" + 0.014*\"loan\" + 0.014*\"owner\" + '\n",
      "  '0.013*\"people\" + 0.013*\"remember\"'),\n",
      " (2,\n",
      "  '0.033*\"fever\" + 0.024*\"go\" + 0.019*\"economy\" + 0.018*\"aunt\" + '\n",
      "  '0.018*\"foreign\" + 0.017*\"live\" + 0.013*\"city\" + 0.013*\"company\" + '\n",
      "  '0.012*\"everywhere\" + 0.012*\"reduce\"'),\n",
      " (3,\n",
      "  '0.050*\"live\" + 0.043*\"month\" + 0.039*\"people\" + 0.034*\"close\" + '\n",
      "  '0.030*\"good\" + 0.030*\"city\" + 0.027*\"clean\" + 0.025*\"number\" + '\n",
      "  '0.022*\"blockade\" + 0.022*\"life\"')]\n",
      "[(0,\n",
      "  '0.043*\"treat\" + 0.039*\"patient\" + 0.025*\"treatment\" + 0.018*\"reveal\" + '\n",
      "  '0.018*\"hospital\" + 0.017*\"medical\" + 0.016*\"great\" + 0.012*\"fall\" + '\n",
      "  '0.010*\"diagnosis\" + 0.010*\"care\"'),\n",
      " (1,\n",
      "  '0.039*\"city\" + 0.039*\"people\" + 0.030*\"country\" + 0.027*\"close\" + '\n",
      "  '0.026*\"closure\" + 0.017*\"take\" + 0.017*\"control\" + 0.013*\"government\" + '\n",
      "  '0.013*\"spread\" + 0.013*\"day\"'),\n",
      " (2,\n",
      "  '0.029*\"return\" + 0.027*\"prevention\" + 0.024*\"video\" + 0.019*\"announce\" + '\n",
      "  '0.019*\"guy\" + 0.018*\"australian\" + 0.015*\"hubei\" + 0.014*\"continue\" + '\n",
      "  '0.014*\"blockade\" + 0.013*\"lift\"'),\n",
      " (3,\n",
      "  '0.026*\"work\" + 0.020*\"lose\" + 0.018*\"pain\" + 0.018*\"construction\" + '\n",
      "  '0.017*\"diamond_princess\" + 0.015*\"humanely\" + 0.014*\"life\" + 0.013*\"live\" + '\n",
      "  '0.009*\"affect\" + 0.009*\"resumption\"')]\n",
      "[(0,\n",
      "  '0.052*\"people\" + 0.050*\"city\" + 0.040*\"close\" + 0.034*\"many\" + '\n",
      "  '0.027*\"country\" + 0.022*\"money\" + 0.021*\"know\" + 0.016*\"go\" + 0.013*\"live\" '\n",
      "  '+ 0.012*\"food\"'),\n",
      " (1,\n",
      "  '0.051*\"closure\" + 0.026*\"city\" + 0.018*\"control\" + 0.018*\"due\" + '\n",
      "  '0.016*\"time\" + 0.012*\"day\" + 0.012*\"year\" + 0.010*\"lot\" + 0.009*\"outbreak\" '\n",
      "  '+ 0.009*\"next\"'),\n",
      " (2,\n",
      "  '0.029*\"prevention\" + 0.024*\"party\" + 0.024*\"say\" + 0.016*\"situation\" + '\n",
      "  '0.016*\"current\" + 0.011*\"policy\" + 0.011*\"talk\" + 0.010*\"closure\" + '\n",
      "  '0.009*\"government\" + 0.009*\"bad\"'),\n",
      " (3,\n",
      "  '0.032*\"death\" + 0.029*\"people\" + 0.028*\"die\" + 0.021*\"number\" + '\n",
      "  '0.018*\"cause\" + 0.017*\"disaster\" + 0.017*\"use\" + 0.015*\"economy\" + '\n",
      "  '0.012*\"starve\" + 0.011*\"official\"')]\n"
     ]
    }
   ],
   "source": [
    "def keyword_in_topics(period):\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "\n",
    "    # Print the Keyword in the 10 topics\n",
    "    pprint(lda_model.print_topics())\n",
    "    doc_lda = lda_model[corpus]\n",
    "    tweets[period][\"doc_lda\"] = doc_lda\n",
    "\n",
    "for period in tweets.keys():\n",
    "    keyword_in_topics(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e5cfb880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x00000175D41758B0>\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"doc_lda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d2763a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_coherence(period):\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    data_lemmatized = tweets[period][\"data_lemmatized\"]\n",
    "    id2word = tweets[period][\"id2word\"]\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "\n",
    "    # Compute Perplexity\n",
    "    tweets[period][\"perplexity\"] = lda_model.log_perplexity(corpus)\n",
    "    # print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "    tweets[period][\"coherence_model_lda\"] = coherence_model_lda\n",
    "    tweets[period][\"coherence_lda\"] = coherence_lda\n",
    "\n",
    "    # print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "for period in tweets.keys():\n",
    "    perplexity_coherence(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0170540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.721058392952905\n",
      "0.3630860495547645\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"perplexity\"])\n",
    "print(tweets[list(tweets.keys())[index]][\"coherence_lda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8cf279ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_model(period): \n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "\n",
    "    optimal_model = lda_model\n",
    "    model_topics = optimal_model.show_topics(formatted=False)\n",
    "\n",
    "    tweets[period][\"model_topics\"] = model_topics\n",
    "\n",
    "for period in tweets.keys():\n",
    "    optimal_model(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2158d6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [('week', 0.015394689), ('news', 0.011458637), ('life', 0.010509352), ('post', 0.010279826), ('call', 0.01011382), ('second', 0.009262819), ('force', 0.009057287), ('disease', 0.00899129), ('video', 0.008359657), ('mask', 0.008124177)]), (1, [('people', 0.04904595), ('day', 0.041766297), ('case', 0.02298288), ('city', 0.019750832), ('town', 0.016668594), ('country', 0.016286071), ('outbreak', 0.0151690105), ('resident', 0.014788403), ('new', 0.014545913), ('say', 0.013076739)]), (2, [('ncov', 0.023394438), ('put', 0.021885443), ('spread', 0.018528098), ('leave', 0.016608331), ('case', 0.015789146), ('first', 0.013514718), ('quarantine', 0.013407771), ('population', 0.013169179), ('virus', 0.012353554), ('due', 0.012002673)]), (3, [('report', 0.019369626), ('quarantine', 0.018293725), ('measure', 0.015736118), ('take', 0.015252979), ('apartment_amidst', 0.011982421), ('amp', 0.010973084), ('novel', 0.010697662), ('city', 0.010030347), ('base', 0.009814688), ('outbreak', 0.009599294)])]\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"model_topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "31465ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, atopic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "953b7b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "covid_lockdown\n",
      "Topic 0 |---------------------\n",
      "\n",
      "year                 0.067\n",
      "first                0.035\n",
      "today                0.020\n",
      "government           0.019\n",
      "come                 0.016\n",
      "issue                0.015\n",
      "third                0.015\n",
      "see                  0.012\n",
      "series               0.012\n",
      "due                  0.012\n",
      "Topic 1 |---------------------\n",
      "\n",
      "support              0.025\n",
      "happen               0.018\n",
      "mental               0.017\n",
      "former               0.016\n",
      "boris_johnson        0.016\n",
      "hlth                 0.015\n",
      "collect              0.015\n",
      "expenditure_data     0.014\n",
      "party                0.013\n",
      "truth                0.013\n",
      "Topic 2 |---------------------\n",
      "\n",
      "make                 0.022\n",
      "impact               0.021\n",
      "post                 0.017\n",
      "population           0.017\n",
      "mask                 0.016\n",
      "wear                 0.016\n",
      "inequality           0.015\n",
      "people               0.014\n",
      "world                0.012\n",
      "lockdown             0.011\n",
      "Topic 3 |---------------------\n",
      "\n",
      "first                0.055\n",
      "year                 0.043\n",
      "day                  0.041\n",
      "anniversary          0.035\n",
      "today                0.030\n",
      "die                  0.025\n",
      "national             0.025\n",
      "mark                 0.024\n",
      "time                 0.020\n",
      "remember             0.019\n",
      "term                 frequency\n",
      "\n",
      "jan_mar\n",
      "Topic 0 |---------------------\n",
      "\n",
      "week                 0.015\n",
      "news                 0.011\n",
      "life                 0.011\n",
      "post                 0.010\n",
      "call                 0.010\n",
      "second               0.009\n",
      "force                0.009\n",
      "disease              0.009\n",
      "video                0.008\n",
      "mask                 0.008\n",
      "Topic 1 |---------------------\n",
      "\n",
      "people               0.049\n",
      "day                  0.042\n",
      "case                 0.023\n",
      "city                 0.020\n",
      "town                 0.017\n",
      "country              0.016\n",
      "outbreak             0.015\n",
      "resident             0.015\n",
      "new                  0.015\n",
      "say                  0.013\n",
      "Topic 2 |---------------------\n",
      "\n",
      "ncov                 0.023\n",
      "put                  0.022\n",
      "spread               0.019\n",
      "leave                0.017\n",
      "case                 0.016\n",
      "first                0.014\n",
      "quarantine           0.013\n",
      "population           0.013\n",
      "virus                0.012\n",
      "due                  0.012\n",
      "Topic 3 |---------------------\n",
      "\n",
      "report               0.019\n",
      "quarantine           0.018\n",
      "measure              0.016\n",
      "take                 0.015\n",
      "apartment_amidst     0.012\n",
      "amp                  0.011\n",
      "novel                0.011\n",
      "city                 0.010\n",
      "base                 0.010\n",
      "outbreak             0.010\n",
      "term                 frequency\n",
      "\n",
      "mar\n",
      "Topic 0 |---------------------\n",
      "\n",
      "first                0.052\n",
      "indian               0.047\n",
      "time                 0.022\n",
      "quarantine           0.016\n",
      "government           0.013\n",
      "coronaupdate         0.011\n",
      "country              0.011\n",
      "good                 0.011\n",
      "food                 0.010\n",
      "world                0.010\n",
      "Topic 1 |---------------------\n",
      "\n",
      "tune                 0.047\n",
      "drive                0.046\n",
      "exclusive            0.037\n",
      "skoda                0.037\n",
      "soil                 0.037\n",
      "home                 0.019\n",
      "help                 0.014\n",
      "stay                 0.012\n",
      "fight                0.008\n",
      "live                 0.008\n",
      "Topic 2 |---------------------\n",
      "\n",
      "right                0.045\n",
      "catch                0.041\n",
      "people               0.028\n",
      "need                 0.014\n",
      "case                 0.013\n",
      "go                   0.012\n",
      "due                  0.012\n",
      "say                  0.010\n",
      "amp                  0.010\n",
      "take                 0.009\n",
      "Topic 3 |---------------------\n",
      "\n",
      "day                  0.044\n",
      "stayhome             0.020\n",
      "total                0.013\n",
      "state                0.012\n",
      "virus                0.011\n",
      "spread               0.010\n",
      "come                 0.010\n",
      "stop                 0.009\n",
      "make                 0.009\n",
      "period               0.008\n",
      "term                 frequency\n",
      "\n",
      "may_nov\n",
      "Topic 0 |---------------------\n",
      "\n",
      "first                0.052\n",
      "indian               0.047\n",
      "time                 0.022\n",
      "quarantine           0.016\n",
      "government           0.013\n",
      "coronaupdate         0.011\n",
      "country              0.011\n",
      "good                 0.011\n",
      "food                 0.010\n",
      "world                0.010\n",
      "Topic 1 |---------------------\n",
      "\n",
      "tune                 0.047\n",
      "drive                0.046\n",
      "exclusive            0.037\n",
      "skoda                0.037\n",
      "soil                 0.037\n",
      "home                 0.019\n",
      "help                 0.014\n",
      "stay                 0.012\n",
      "fight                0.008\n",
      "live                 0.008\n",
      "Topic 2 |---------------------\n",
      "\n",
      "right                0.045\n",
      "catch                0.041\n",
      "people               0.028\n",
      "need                 0.014\n",
      "case                 0.013\n",
      "go                   0.012\n",
      "due                  0.012\n",
      "say                  0.010\n",
      "amp                  0.010\n",
      "take                 0.009\n",
      "Topic 3 |---------------------\n",
      "\n",
      "day                  0.044\n",
      "stayhome             0.020\n",
      "total                0.013\n",
      "state                0.012\n",
      "virus                0.011\n",
      "spread               0.010\n",
      "come                 0.010\n",
      "stop                 0.009\n",
      "make                 0.009\n",
      "period               0.008\n",
      "term                 frequency\n",
      "\n",
      "cn_covid_lockdown\n",
      "Topic 0 |---------------------\n",
      "\n",
      "closure              0.030\n",
      "people               0.025\n",
      "city                 0.021\n",
      "first                0.018\n",
      "year                 0.017\n",
      "time                 0.013\n",
      "even                 0.011\n",
      "say                  0.011\n",
      "die                  0.010\n",
      "leave                0.009\n",
      "Topic 1 |---------------------\n",
      "\n",
      "city                 0.035\n",
      "year                 0.035\n",
      "big                  0.027\n",
      "business             0.025\n",
      "closure              0.022\n",
      "even                 0.018\n",
      "loan                 0.014\n",
      "owner                0.014\n",
      "people               0.013\n",
      "remember             0.013\n",
      "Topic 2 |---------------------\n",
      "\n",
      "fever                0.033\n",
      "go                   0.024\n",
      "economy              0.019\n",
      "aunt                 0.018\n",
      "foreign              0.018\n",
      "live                 0.017\n",
      "city                 0.013\n",
      "company              0.013\n",
      "everywhere           0.012\n",
      "reduce               0.012\n",
      "Topic 3 |---------------------\n",
      "\n",
      "live                 0.050\n",
      "month                0.043\n",
      "people               0.039\n",
      "close                0.034\n",
      "good                 0.030\n",
      "city                 0.030\n",
      "clean                0.027\n",
      "number               0.025\n",
      "blockade             0.022\n",
      "life                 0.022\n",
      "term                 frequency\n",
      "\n",
      "cn_jan_apr_2020\n",
      "Topic 0 |---------------------\n",
      "\n",
      "treat                0.043\n",
      "patient              0.039\n",
      "treatment            0.025\n",
      "reveal               0.018\n",
      "hospital             0.018\n",
      "medical              0.017\n",
      "great                0.016\n",
      "fall                 0.012\n",
      "diagnosis            0.010\n",
      "care                 0.010\n",
      "Topic 1 |---------------------\n",
      "\n",
      "city                 0.039\n",
      "people               0.039\n",
      "country              0.030\n",
      "close                0.027\n",
      "closure              0.026\n",
      "take                 0.017\n",
      "control              0.017\n",
      "government           0.013\n",
      "spread               0.013\n",
      "day                  0.013\n",
      "Topic 2 |---------------------\n",
      "\n",
      "return               0.029\n",
      "prevention           0.027\n",
      "video                0.024\n",
      "announce             0.019\n",
      "guy                  0.019\n",
      "australian           0.018\n",
      "hubei                0.015\n",
      "continue             0.014\n",
      "blockade             0.014\n",
      "lift                 0.013\n",
      "Topic 3 |---------------------\n",
      "\n",
      "work                 0.026\n",
      "lose                 0.020\n",
      "pain                 0.018\n",
      "construction         0.018\n",
      "diamond_princess     0.017\n",
      "humanely             0.015\n",
      "life                 0.014\n",
      "live                 0.013\n",
      "affect               0.009\n",
      "resumption           0.009\n",
      "term                 frequency\n",
      "\n",
      "cn_may_june_2022\n",
      "Topic 0 |---------------------\n",
      "\n",
      "people               0.052\n",
      "city                 0.050\n",
      "close                0.040\n",
      "many                 0.034\n",
      "country              0.027\n",
      "money                0.022\n",
      "know                 0.021\n",
      "go                   0.016\n",
      "live                 0.013\n",
      "food                 0.012\n",
      "Topic 1 |---------------------\n",
      "\n",
      "closure              0.051\n",
      "city                 0.026\n",
      "control              0.018\n",
      "due                  0.018\n",
      "time                 0.016\n",
      "day                  0.012\n",
      "year                 0.012\n",
      "lot                  0.010\n",
      "outbreak             0.009\n",
      "next                 0.009\n",
      "Topic 2 |---------------------\n",
      "\n",
      "prevention           0.029\n",
      "party                0.024\n",
      "say                  0.024\n",
      "situation            0.016\n",
      "current              0.016\n",
      "policy               0.011\n",
      "talk                 0.011\n",
      "closure              0.010\n",
      "government           0.009\n",
      "bad                  0.009\n",
      "Topic 3 |---------------------\n",
      "\n",
      "death                0.032\n",
      "people               0.029\n",
      "die                  0.028\n",
      "number               0.021\n",
      "cause                0.018\n",
      "disaster             0.017\n",
      "use                  0.017\n",
      "economy              0.015\n",
      "starve               0.012\n",
      "official             0.011\n"
     ]
    }
   ],
   "source": [
    "def explore_topic_by_title(period):\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "    topic_summaries = []\n",
    "\n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "    print(period)\n",
    "    for i in range(4):\n",
    "        print('Topic '+str(i)+' |---------------------\\n')\n",
    "        tmp = explore_topic(lda_model,topic_number=i, topn=10, output=True )\n",
    "        #     print tmp[:5]\n",
    "        topic_summaries += [tmp[:5]]\n",
    "        # print\n",
    "\n",
    "    tweets[period][\"topic_summaries\"] = topic_summaries\n",
    "\n",
    "for period in tweets.keys():\n",
    "    explore_topic_by_title(period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6590af24",
   "metadata": {},
   "source": [
    "<h1>Data Output - Dominant Topic</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2a224b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "for period in tweets.keys():\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    texts = tweets[period][\"texts\"]\n",
    "    \n",
    "    df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, texts)\n",
    "    \n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    \n",
    "    tweets[period][\"df_dominant_topic\"] = df_dominant_topic\n",
    "    \n",
    "    df_dominant_topic.to_csv(output_path + period + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4270bace",
   "metadata": {},
   "source": [
    "<h1>Data Visualization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16757d8b",
   "metadata": {},
   "source": [
    "<h3>PyLDAVis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a5cc48d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lindy\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "# Visualize the topics\n",
    "for period in tweets.keys():\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    id2word = tweets[period][\"id2word\"]\n",
    "\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    \n",
    "    file_path = visuals_output_path + period + '.html'\n",
    "    pyLDAvis.save_html(LDAvis_prepared, file_path)\n",
    "    \n",
    "    with open(file_path, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write(\"<h1>\" + period + \"</h1>\" + '\\n' + content)\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
