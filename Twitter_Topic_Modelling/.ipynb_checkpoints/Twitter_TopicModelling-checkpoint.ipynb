{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83dbc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../Twitter_Data/\"\n",
    "cn_folder_path = \"../Twitter_Data/Twitter_Data_Chinese/\"\n",
    "\n",
    "visuals_output_path = \"./visuals/\"\n",
    "output_path = \"./output/\"\n",
    "\n",
    "file1 = \"Twitter_Covid-19_Lockdown_5000.csv\"\n",
    "file2 = \"Twitter_Jan_Mar_5000.csv\"\n",
    "file3 = \"Twitter_Mar_5000.csv\"\n",
    "file4 = \"Twitter_May_Nov_5000.csv\"\n",
    "\n",
    "cn_file1 = \"en.Twitter_Covid-19_Lockdown_5000_chinese.csv\"\n",
    "cn_file2 = \"en.Twitter_Jan_Apr_2020_5000_chinese.csv\"\n",
    "cn_file3 = \"en.Twitter_May_June_2022_5000_chinese.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23957281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\lindy\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.7.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: funcy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.18)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (58.0.4)\n",
      "Requirement already satisfied: pandas>=1.3.4 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: numexpr in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.22.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.3.4->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b145bc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lindy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords') #download if don't have yet\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer # tokenize words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abc653eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in files\n",
    "df1 = pd.read_csv(folder_path + file1)\n",
    "df2 = pd.read_csv(folder_path + file2)\n",
    "df3 = pd.read_csv(folder_path + file3)\n",
    "df4 = pd.read_csv(folder_path + file3)\n",
    "\n",
    "cn_df1 = pd.read_csv(cn_folder_path + cn_file1)\n",
    "cn_df2 = pd.read_csv(cn_folder_path + cn_file2)\n",
    "cn_df3 = pd.read_csv(cn_folder_path + cn_file3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05315a87",
   "metadata": {},
   "source": [
    "<h1>Cleaning Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09624baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns mentioning Bheed trailer\n",
    "df1 = df1[df1[\"text\"].str.contains(\"Bheed\")==False]\n",
    "df2 = df2[df2[\"Text\"].str.contains(\"Bheed\")==False]\n",
    "df3 = df3[df3[\"Text\"].str.contains(\"Bheed\")==False]\n",
    "df4 = df4[df4[\"Text\"].str.contains(\"Bheed\")==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7d03793",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = df1[\"text\"].tolist()\n",
    "list2 = df2[\"Text\"].tolist()\n",
    "list3 = df3[\"Text\"].tolist()\n",
    "list4 = df4[\"Text\"].tolist()\n",
    "\n",
    "cn_list1 = cn_df1[\"text\"].tolist()\n",
    "cn_list2 = cn_df2[\"text\"].tolist()\n",
    "cn_list3 = cn_df3[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11ee88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_user_mentions(text):\n",
    "    return re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_digit_strings(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    remove_chars = '[0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "    return re.sub(remove_chars, ' ', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    result_text = text\n",
    "    result_text = remove_user_mentions(result_text)\n",
    "    result_text = remove_links(result_text)\n",
    "    result_text = remove_digit_strings(result_text)\n",
    "    result_text = remove_special_chars(result_text)\n",
    "    result_text = result_text.lower()\n",
    "    return result_text\n",
    "\n",
    "def remove_cn_chars(text):\n",
    "    result_text = re.sub(r'([\\u4e00-\\u9fff]+', '', text)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3051b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list1)):\n",
    "    list1[i] = clean_text(list1[i])\n",
    "\n",
    "for i in range(len(list2)):\n",
    "    list2[i] = clean_text(list2[i])\n",
    "\n",
    "for i in range(len(list3)):\n",
    "    list3[i] = clean_text(list3[i])\n",
    "    \n",
    "for i in range(len(list4)):\n",
    "    list4[i] = clean_text(list4[i])\n",
    "    \n",
    "for i in range(len(cn_list1)):\n",
    "#     cn_list1[i] = remove_cn_chars(cn_list1[i])\n",
    "    cn_list1[i] = clean_text(cn_list1[i])\n",
    "\n",
    "# for i in range(len(cn_list2)):\n",
    "#     cn_list2[i] = remove_cn_chars(cn_list2[i])\n",
    "#     cn_list2[i] = clean_text(cn_list2[i])\n",
    "\n",
    "# for i in range(len(cn_list3)):\n",
    "#     cn_list3[i] = remove_cn_chars(cn_list3[i])\n",
    "#     cn_list3[i] = clean_text(cn_list3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c428790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"cleaned_text\"] = list1\n",
    "df2[\"cleaned_text\"] = list2\n",
    "df3[\"cleaned_text\"] = list3\n",
    "df4[\"cleaned_text\"] = list4\n",
    "\n",
    "cn_df1[\"cleaned_text\"] = cn_list1\n",
    "cn_df2[\"cleaned_text\"] = cn_list2\n",
    "cn_df3[\"cleaned_text\"] = cn_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e195513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of period of tweets to information associated with tweet\n",
    "\n",
    "tweets = {}\n",
    "keys = [\"covid_lockdown\", \"jan_mar\", \"mar\", \"may_nov\", \"cn_covid_lockdown\"]\n",
    "for key in keys:\n",
    "    tweets[key] = {}\n",
    "\n",
    "tweets[\"covid_lockdown\"][\"text\"] = df1[\"cleaned_text\"].tolist()\n",
    "tweets[\"jan_mar\"][\"text\"] = df2[\"cleaned_text\"].tolist()\n",
    "tweets[\"mar\"][\"text\"] = df3[\"cleaned_text\"].tolist()\n",
    "tweets[\"may_nov\"][\"text\"] = df4[\"cleaned_text\"].tolist()\n",
    "\n",
    "tweets[\"cn_covid_lockdown\"][\"text\"] = cn_df1[\"cleaned_text\"].tolist()\n",
    "# tweets[\"cn_jan_apr_2020\"][\"text\"] = cn_df2[\"cleaned_text\"].tolist()\n",
    "# tweets[\"cn_may_june_2022\"][\"text\"] = cn_df3[\"cleaned_text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127edb4",
   "metadata": {},
   "source": [
    "<h1>Tokenizing Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "015218ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get custom stopwords\n",
    "content = []\n",
    "f = open(\"./stopwords.txt\", encoding = 'utf-8')\n",
    "# perform file operations\n",
    "for line in f:\n",
    "    content.append(line)\n",
    "f.close()\n",
    "\n",
    "custom_stopwords = []\n",
    "for line in content:\n",
    "    wordlist = line.split(\",\")\n",
    "    for word in wordlist:\n",
    "        custom_stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5fb4491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "exclude_words = stop_words\n",
    "\n",
    "exclude_words_extra = [\"RT\",\"still\",\"covid\",\"coronavirus\",\"lockdown\",\"lockdo\",\"pandemic\",\"let\",\"get\",\"ago\",\"go\",\"im\",\"ive\",\"would\",\"one\",\"also\",\"to\",\"tag\"]\n",
    "\n",
    "# Exclude custom stopwords\n",
    "exclude_words.extend(custom_stopwords)\n",
    "exclude_words.extend(exclude_words_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "645a7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "for period in tweets.keys():\n",
    "    tweets[period][\"data_words\"] = list(sent_to_words(tweets[period][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28ce038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_trigram_models(period):\n",
    "    data_words = tweets[period][\"data_words\"]\n",
    "    \n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "    \n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    tweets[period][\"bigram\"] = bigram\n",
    "    tweets[period][\"trigram\"] = trigram\n",
    "    tweets[period][\"bigram_mod\"] = bigram_mod\n",
    "    tweets[period][\"trigram_mod\"] = trigram_mod\n",
    "\n",
    "for period in tweets.keys():\n",
    "    bigram_trigram_models(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0fb35563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in exclude_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts, bigram_mod, trigram_mod):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d11d7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined(period):\n",
    "    data_words = tweets[period][\"data_words\"]\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "    data_words_nostops = data_words\n",
    "    tweets[period][\"data_words_nostops\"] = data_words_nostops \n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops, tweets[period][\"bigram_mod\"])\n",
    "    tweets[period][\"data_words_bigrams\"] = data_words_bigrams\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    tweets[period][\"data_lemmatized\"] = data_lemmatized\n",
    "\n",
    "for period in tweets.keys():\n",
    "    combined(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "11b440e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'data_words', 'bigram', 'trigram', 'bigram_mod', 'trigram_mod', 'data_words_nostops', 'data_words_bigrams', 'data_lemmatized'])\n"
     ]
    }
   ],
   "source": [
    "print(tweets[list(tweets.keys())[0]].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54af5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(period):\n",
    "    data_lemmatized = tweets[period][\"data_lemmatized\"]\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "    tweets[period][\"id2word\"] = id2word\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "    tweets[period][\"texts\"] = texts\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    tweets[period][\"corpus\"] = corpus\n",
    "\n",
    "    # Human readable format of corpus (term-frequency)\n",
    "    tweets[period][\"corpus_readable\"] = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "for period in tweets.keys():\n",
    "    tokenize(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e8c0afaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('covid', 1), ('home', 1), ('lockdown', 1), ('place', 1)]]\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"corpus_readable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2800f0c",
   "metadata": {},
   "source": [
    "<h1>LDA Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2314087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "# supporting function 2\n",
    "def compute_perplexity_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    perplexity_score = lda_model.log_perplexity(corpus_sets[i])\n",
    "    \n",
    "    return perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6cb92fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(period):\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    id2word = tweets[period][\"id2word\"]\n",
    "\n",
    "    # Build LDA model\n",
    "    num_topics = 4\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=num_topics, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "    tweets[period][\"lda_model\"] = lda_model\n",
    "\n",
    "for period in tweets.keys():\n",
    "    lda_model(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e7fa97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel<num_terms=3084, num_topics=4, decay=0.5, chunksize=100>\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"lda_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa2de5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.103*\"lockdown\" + 0.082*\"covid\" + 0.058*\"year\" + 0.047*\"first\" + '\n",
      "  '0.029*\"support\" + 0.026*\"today\" + 0.021*\"anniversary\" + 0.020*\"mark\" + '\n",
      "  '0.016*\"third\" + 0.015*\"pandemic\"'),\n",
      " (1,\n",
      "  '0.055*\"lockdown\" + 0.053*\"covid\" + 0.019*\"amp\" + 0.017*\"more\" + '\n",
      "  '0.016*\"impact\" + 0.012*\"mask\" + 0.011*\"wear\" + 0.011*\"inequality\" + '\n",
      "  '0.010*\"expenditure_data\" + 0.010*\"collected_by\"'),\n",
      " (2,\n",
      "  '0.036*\"covid\" + 0.030*\"lockdown\" + 0.026*\"people\" + 0.026*\"day\" + '\n",
      "  '0.024*\"make\" + 0.022*\"happen\" + 0.021*\"post\" + 0.020*\"mental\" + '\n",
      "  '0.019*\"population\" + 0.017*\"hlth\"'),\n",
      " (3,\n",
      "  '0.055*\"covid\" + 0.054*\"lockdown\" + 0.028*\"former\" + 0.024*\"party\" + '\n",
      "  '0.021*\"boris_johnson\" + 0.020*\"truth\" + 0.019*\"speak\" + 0.017*\"shoot\" + '\n",
      "  '0.017*\"sheep\" + 0.017*\"agenda_klausschw\"')]\n",
      "[(0,\n",
      "  '0.031*\"town\" + 0.013*\"say\" + 0.013*\"base\" + 0.012*\"order\" + 0.010*\"close\" + '\n",
      "  '0.010*\"ban\" + 0.010*\"video\" + 0.009*\"public\" + 0.008*\"escape\" + '\n",
      "  '0.008*\"datum\"'),\n",
      " (1,\n",
      "  '0.023*\"chinese\" + 0.019*\"start\" + 0.015*\"official\" + 0.014*\"see\" + '\n",
      "  '0.013*\"safe\" + 0.012*\"all\" + 0.011*\"include\" + 0.009*\"run\" + '\n",
      "  '0.009*\"million\" + 0.009*\"let\"'),\n",
      " (2,\n",
      "  '0.034*\"case\" + 0.025*\"lockdown\" + 0.020*\"day\" + 0.020*\"covid\" + '\n",
      "  '0.018*\"people\" + 0.015*\"new\" + 0.014*\"life\" + 0.013*\"report\" + '\n",
      "  '0.013*\"coronavirus\" + 0.011*\"first\"'),\n",
      " (3,\n",
      "  '0.111*\"lockdown\" + 0.097*\"covid\" + 0.043*\"coronavirus\" + 0.020*\"city\" + '\n",
      "  '0.018*\"people\" + 0.014*\"ncov\" + 0.013*\"quarantine\" + 0.012*\"outbreak\" + '\n",
      "  '0.011*\"day\" + 0.011*\"now\"')]\n",
      "[(0,\n",
      "  '0.122*\"covid\" + 0.122*\"lockdown\" + 0.020*\"first\" + 0.019*\"tune\" + '\n",
      "  '0.016*\"coronavirus\" + 0.014*\"day\" + 0.012*\"people\" + 0.009*\"time\" + '\n",
      "  '0.008*\"go\" + 0.008*\"home\"'),\n",
      " (1,\n",
      "  '0.056*\"drive\" + 0.055*\"catch\" + 0.052*\"soil\" + 0.052*\"karoq\" + '\n",
      "  '0.052*\"exclusives\" + 0.052*\"skoda\" + 0.009*\"start\" + 0.008*\"do\" + '\n",
      "  '0.008*\"staysafe\" + 0.007*\"support\"'),\n",
      " (2,\n",
      "  '0.013*\"great\" + 0.011*\"covidlockdown\" + 0.009*\"police\" + 0.007*\"nationwide\" '\n",
      "  '+ 0.007*\"ensure\" + 0.007*\"break\" + 0.006*\"house\" + 0.006*\"reach\" + '\n",
      "  '0.005*\"social\" + 0.005*\"selfisolation\"'),\n",
      " (3,\n",
      "  '0.043*\"right_now\" + 0.043*\"indian\" + 0.014*\"need\" + 0.013*\"help\" + '\n",
      "  '0.011*\"only\" + 0.010*\"say\" + 0.010*\"total\" + 0.009*\"virus\" + 0.008*\"more\" + '\n",
      "  '0.008*\"week\"')]\n",
      "[(0,\n",
      "  '0.122*\"covid\" + 0.122*\"lockdown\" + 0.020*\"first\" + 0.019*\"tune\" + '\n",
      "  '0.016*\"coronavirus\" + 0.014*\"day\" + 0.012*\"people\" + 0.009*\"time\" + '\n",
      "  '0.008*\"go\" + 0.008*\"home\"'),\n",
      " (1,\n",
      "  '0.056*\"drive\" + 0.055*\"catch\" + 0.052*\"soil\" + 0.052*\"karoq\" + '\n",
      "  '0.052*\"exclusives\" + 0.052*\"skoda\" + 0.009*\"start\" + 0.008*\"do\" + '\n",
      "  '0.008*\"staysafe\" + 0.007*\"support\"'),\n",
      " (2,\n",
      "  '0.013*\"great\" + 0.011*\"covidlockdown\" + 0.009*\"police\" + 0.007*\"nationwide\" '\n",
      "  '+ 0.007*\"ensure\" + 0.007*\"break\" + 0.006*\"house\" + 0.006*\"reach\" + '\n",
      "  '0.005*\"social\" + 0.005*\"selfisolation\"'),\n",
      " (3,\n",
      "  '0.043*\"right_now\" + 0.043*\"indian\" + 0.014*\"need\" + 0.013*\"help\" + '\n",
      "  '0.011*\"only\" + 0.010*\"say\" + 0.010*\"total\" + 0.009*\"virus\" + 0.008*\"more\" + '\n",
      "  '0.008*\"week\"')]\n",
      "[(0,\n",
      "  '0.059*\"month\" + 0.055*\"people\" + 0.054*\"life\" + 0.031*\"number\" + '\n",
      "  '0.028*\"flight\" + 0.026*\"human\" + 0.026*\"closure\" + 0.026*\"blockade\" + '\n",
      "  '0.026*\"live\" + 0.026*\"today\"'),\n",
      " (1,\n",
      "  '0.029*\"city\" + 0.027*\"people\" + 0.025*\"year\" + 0.023*\"business\" + '\n",
      "  '0.020*\"even\" + 0.019*\"big\" + 0.018*\"chinese\" + 0.016*\"closure\" + '\n",
      "  '0.015*\"economy\" + 0.014*\"memory\"'),\n",
      " (2,\n",
      "  '0.026*\"city\" + 0.026*\"live\" + 0.021*\"good\" + 0.021*\"year\" + 0.019*\"people\" '\n",
      "  '+ 0.019*\"no\" + 0.018*\"clean\" + 0.018*\"long\" + 0.016*\"close\" + '\n",
      "  '0.016*\"epidemic\"'),\n",
      " (3,\n",
      "  '0.025*\"fever\" + 0.018*\"go\" + 0.014*\"foreign\" + 0.014*\"xd\" + 0.013*\"uncle\" + '\n",
      "  '0.013*\"aunt\" + 0.013*\"epidemic\" + 0.013*\"year\" + 0.013*\"again\" + '\n",
      "  '0.012*\"city\"')]\n"
     ]
    }
   ],
   "source": [
    "def keyword_in_topics(period):\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "\n",
    "    # Print the Keyword in the 10 topics\n",
    "    pprint(lda_model.print_topics())\n",
    "    doc_lda = lda_model[corpus]\n",
    "    tweets[period][\"doc_lda\"] = doc_lda\n",
    "\n",
    "for period in tweets.keys():\n",
    "    keyword_in_topics(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5cfb880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x000001BE38EE2940>\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"doc_lda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d2763a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_coherence(period):\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    data_lemmatized = tweets[period][\"data_lemmatized\"]\n",
    "    id2word = tweets[period][\"id2word\"]\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "\n",
    "    # Compute Perplexity\n",
    "    tweets[period][\"perplexity\"] = lda_model.log_perplexity(corpus)\n",
    "    # print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "    tweets[period][\"coherence_model_lda\"] = coherence_model_lda\n",
    "    tweets[period][\"coherence_lda\"] = coherence_lda\n",
    "\n",
    "    # print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "for period in tweets.keys():\n",
    "    perplexity_coherence(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0170540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.136573797604998\n",
      "0.42403609035375367\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"perplexity\"])\n",
    "print(tweets[list(tweets.keys())[index]][\"coherence_lda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8cf279ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_model(period): \n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "\n",
    "    optimal_model = lda_model\n",
    "    model_topics = optimal_model.show_topics(formatted=False)\n",
    "\n",
    "    tweets[period][\"model_topics\"] = model_topics\n",
    "\n",
    "for period in tweets.keys():\n",
    "    optimal_model(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2158d6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [('town', 0.031279), ('say', 0.01269617), ('base', 0.0125529915), ('order', 0.0123109035), ('close', 0.010081722), ('ban', 0.009998816), ('video', 0.009547917), ('public', 0.008669036), ('escape', 0.008173242), ('datum', 0.008055627)]), (1, [('chinese', 0.022847172), ('start', 0.018699147), ('official', 0.015295484), ('see', 0.014387247), ('safe', 0.013070171), ('all', 0.012088214), ('include', 0.011151277), ('run', 0.0093834), ('million', 0.009127393), ('let', 0.009045857)]), (2, [('case', 0.034101836), ('lockdown', 0.02500201), ('day', 0.020175047), ('covid', 0.020119369), ('people', 0.01776956), ('new', 0.015229243), ('life', 0.013526615), ('report', 0.012683687), ('coronavirus', 0.012562215), ('first', 0.011416238)]), (3, [('lockdown', 0.11147557), ('covid', 0.0969804), ('coronavirus', 0.04255527), ('city', 0.019650945), ('people', 0.01765093), ('ncov', 0.013701352), ('quarantine', 0.012985469), ('outbreak', 0.01220372), ('day', 0.01149202), ('now', 0.011431002)])]\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(tweets[list(tweets.keys())[index]][\"model_topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "31465ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, atopic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "953b7b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "covid_lockdown\n",
      "Topic 0 |---------------------\n",
      "\n",
      "lockdown             0.103\n",
      "covid                0.082\n",
      "year                 0.058\n",
      "first                0.047\n",
      "support              0.029\n",
      "today                0.026\n",
      "anniversary          0.021\n",
      "mark                 0.020\n",
      "third                0.016\n",
      "pandemic             0.015\n",
      "Topic 1 |---------------------\n",
      "\n",
      "lockdown             0.055\n",
      "covid                0.053\n",
      "amp                  0.019\n",
      "more                 0.017\n",
      "impact               0.016\n",
      "mask                 0.012\n",
      "wear                 0.011\n",
      "inequality           0.011\n",
      "expenditure_data     0.010\n",
      "collected_by         0.010\n",
      "Topic 2 |---------------------\n",
      "\n",
      "covid                0.036\n",
      "lockdown             0.030\n",
      "people               0.026\n",
      "day                  0.026\n",
      "make                 0.024\n",
      "happen               0.022\n",
      "post                 0.021\n",
      "mental               0.020\n",
      "population           0.019\n",
      "hlth                 0.017\n",
      "Topic 3 |---------------------\n",
      "\n",
      "covid                0.055\n",
      "lockdown             0.054\n",
      "former               0.028\n",
      "party                0.024\n",
      "boris_johnson        0.021\n",
      "truth                0.020\n",
      "speak                0.019\n",
      "shoot                0.017\n",
      "sheep                0.017\n",
      "agenda_klausschw     0.017\n",
      "term                 frequency\n",
      "\n",
      "jan_mar\n",
      "Topic 0 |---------------------\n",
      "\n",
      "town                 0.031\n",
      "say                  0.013\n",
      "base                 0.013\n",
      "order                0.012\n",
      "close                0.010\n",
      "ban                  0.010\n",
      "video                0.010\n",
      "public               0.009\n",
      "escape               0.008\n",
      "datum                0.008\n",
      "Topic 1 |---------------------\n",
      "\n",
      "chinese              0.023\n",
      "start                0.019\n",
      "official             0.015\n",
      "see                  0.014\n",
      "safe                 0.013\n",
      "all                  0.012\n",
      "include              0.011\n",
      "run                  0.009\n",
      "million              0.009\n",
      "let                  0.009\n",
      "Topic 2 |---------------------\n",
      "\n",
      "case                 0.034\n",
      "lockdown             0.025\n",
      "day                  0.020\n",
      "covid                0.020\n",
      "people               0.018\n",
      "new                  0.015\n",
      "life                 0.014\n",
      "report               0.013\n",
      "coronavirus          0.013\n",
      "first                0.011\n",
      "Topic 3 |---------------------\n",
      "\n",
      "lockdown             0.111\n",
      "covid                0.097\n",
      "coronavirus          0.043\n",
      "city                 0.020\n",
      "people               0.018\n",
      "ncov                 0.014\n",
      "quarantine           0.013\n",
      "outbreak             0.012\n",
      "day                  0.011\n",
      "now                  0.011\n",
      "term                 frequency\n",
      "\n",
      "mar\n",
      "Topic 0 |---------------------\n",
      "\n",
      "covid                0.122\n",
      "lockdown             0.122\n",
      "first                0.020\n",
      "tune                 0.019\n",
      "coronavirus          0.016\n",
      "day                  0.014\n",
      "people               0.012\n",
      "time                 0.009\n",
      "go                   0.008\n",
      "home                 0.008\n",
      "Topic 1 |---------------------\n",
      "\n",
      "drive                0.056\n",
      "catch                0.055\n",
      "soil                 0.052\n",
      "karoq                0.052\n",
      "exclusives           0.052\n",
      "skoda                0.052\n",
      "start                0.009\n",
      "do                   0.008\n",
      "staysafe             0.008\n",
      "support              0.007\n",
      "Topic 2 |---------------------\n",
      "\n",
      "great                0.013\n",
      "covidlockdown        0.011\n",
      "police               0.009\n",
      "nationwide           0.007\n",
      "ensure               0.007\n",
      "break                0.007\n",
      "house                0.006\n",
      "reach                0.006\n",
      "social               0.005\n",
      "selfisolation        0.005\n",
      "Topic 3 |---------------------\n",
      "\n",
      "right_now            0.043\n",
      "indian               0.043\n",
      "need                 0.014\n",
      "help                 0.013\n",
      "only                 0.011\n",
      "say                  0.010\n",
      "total                0.010\n",
      "virus                0.009\n",
      "more                 0.008\n",
      "week                 0.008\n",
      "term                 frequency\n",
      "\n",
      "may_nov\n",
      "Topic 0 |---------------------\n",
      "\n",
      "covid                0.122\n",
      "lockdown             0.122\n",
      "first                0.020\n",
      "tune                 0.019\n",
      "coronavirus          0.016\n",
      "day                  0.014\n",
      "people               0.012\n",
      "time                 0.009\n",
      "go                   0.008\n",
      "home                 0.008\n",
      "Topic 1 |---------------------\n",
      "\n",
      "drive                0.056\n",
      "catch                0.055\n",
      "soil                 0.052\n",
      "karoq                0.052\n",
      "exclusives           0.052\n",
      "skoda                0.052\n",
      "start                0.009\n",
      "do                   0.008\n",
      "staysafe             0.008\n",
      "support              0.007\n",
      "Topic 2 |---------------------\n",
      "\n",
      "great                0.013\n",
      "covidlockdown        0.011\n",
      "police               0.009\n",
      "nationwide           0.007\n",
      "ensure               0.007\n",
      "break                0.007\n",
      "house                0.006\n",
      "reach                0.006\n",
      "social               0.005\n",
      "selfisolation        0.005\n",
      "Topic 3 |---------------------\n",
      "\n",
      "right_now            0.043\n",
      "indian               0.043\n",
      "need                 0.014\n",
      "help                 0.013\n",
      "only                 0.011\n",
      "say                  0.010\n",
      "total                0.010\n",
      "virus                0.009\n",
      "more                 0.008\n",
      "week                 0.008\n",
      "term                 frequency\n",
      "\n",
      "cn_covid_lockdown\n",
      "Topic 0 |---------------------\n",
      "\n",
      "month                0.059\n",
      "people               0.055\n",
      "life                 0.054\n",
      "number               0.031\n",
      "flight               0.028\n",
      "human                0.026\n",
      "closure              0.026\n",
      "blockade             0.026\n",
      "live                 0.026\n",
      "today                0.026\n",
      "Topic 1 |---------------------\n",
      "\n",
      "city                 0.029\n",
      "people               0.027\n",
      "year                 0.025\n",
      "business             0.023\n",
      "even                 0.020\n",
      "big                  0.019\n",
      "chinese              0.018\n",
      "closure              0.016\n",
      "economy              0.015\n",
      "memory               0.014\n",
      "Topic 2 |---------------------\n",
      "\n",
      "city                 0.026\n",
      "live                 0.026\n",
      "good                 0.021\n",
      "year                 0.021\n",
      "people               0.019\n",
      "no                   0.019\n",
      "clean                0.018\n",
      "long                 0.018\n",
      "close                0.016\n",
      "epidemic             0.016\n",
      "Topic 3 |---------------------\n",
      "\n",
      "fever                0.025\n",
      "go                   0.018\n",
      "foreign              0.014\n",
      "xd                   0.014\n",
      "uncle                0.013\n",
      "aunt                 0.013\n",
      "epidemic             0.013\n",
      "year                 0.013\n",
      "again                0.013\n",
      "city                 0.012\n"
     ]
    }
   ],
   "source": [
    "def explore_topic_by_title(period):\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "    topic_summaries = []\n",
    "\n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "    print(period)\n",
    "    for i in range(4):\n",
    "        print('Topic '+str(i)+' |---------------------\\n')\n",
    "        tmp = explore_topic(lda_model,topic_number=i, topn=10, output=True )\n",
    "        #     print tmp[:5]\n",
    "        topic_summaries += [tmp[:5]]\n",
    "        # print\n",
    "\n",
    "    tweets[period][\"topic_summaries\"] = topic_summaries\n",
    "\n",
    "for period in tweets.keys():\n",
    "    explore_topic_by_title(period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6590af24",
   "metadata": {},
   "source": [
    "<h1>Data Output - Dominant Topic</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a224b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "for period in tweets.keys():\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    texts = tweets[period][\"texts\"]\n",
    "    \n",
    "    df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, texts)\n",
    "    \n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    \n",
    "    tweets[period][\"df_dominant_topic\"] = df_dominant_topic\n",
    "    \n",
    "    df_dominant_topic.to_csv(output_path + period + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4270bace",
   "metadata": {},
   "source": [
    "<h1>Data Visualization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16757d8b",
   "metadata": {},
   "source": [
    "<h3>PyLDAVis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a5cc48d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lindy\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "# Visualize the topics\n",
    "for period in tweets.keys():\n",
    "    lda_model = tweets[period][\"lda_model\"]\n",
    "    corpus = tweets[period][\"corpus\"]\n",
    "    id2word = tweets[period][\"id2word\"]\n",
    "\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    \n",
    "    file_path = visuals_output_path + period + '.html'\n",
    "    pyLDAvis.save_html(LDAvis_prepared, file_path)\n",
    "    \n",
    "    with open(file_path, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write(\"<h1>\" + period + \"</h1>\" + '\\n' + content)\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
