{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0d3e61",
   "metadata": {},
   "source": [
    "<h1>Twitter Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f895a4b",
   "metadata": {},
   "source": [
    "<h3>Installation and import of libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e8faa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\lindy\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.7.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.22.4)\n",
      "Requirement already satisfied: numexpr in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (58.0.4)\n",
      "Requirement already satisfied: pandas>=1.3.4 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.4)\n",
      "Requirement already satisfied: funcy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.18)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.3.4->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05608374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lindy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords') #download if don't have yet\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer # tokenize words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65fd905",
   "metadata": {},
   "source": [
    "<h3>Viewing Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a6a59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "folder_path = \"../Twitter_Data/\"\n",
    "file1 = \"Twitter_Covid-19_Lockdown_5000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1406c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>id</th>\n",
       "      <th>edit_history_tweet_ids</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>withheld.copyright</th>\n",
       "      <th>withheld.country_codes</th>\n",
       "      <th>author_name</th>\n",
       "      <th>orginal_text</th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>495071269</td>\n",
       "      <td>1640204550631043072</td>\n",
       "      <td>1640204550631043072</td>\n",
       "      <td>['1640204550631043072']</td>\n",
       "      <td>RT @Chris_EvansMP: Today marks three years sin...</td>\n",
       "      <td>2023-03-27T04:10:13.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BarnettElaine</td>\n",
       "      <td>Today marks three years since the UK went int...</td>\n",
       "      <td>Today marks three years since the UK went int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1564131612505149440</td>\n",
       "      <td>1639919857012469760</td>\n",
       "      <td>1640204157683417089</td>\n",
       "      <td>['1640204157683417089']</td>\n",
       "      <td>@RoastSmith_ I used play Fortnite a lot on my ...</td>\n",
       "      <td>2023-03-27T04:08:39.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tanseus1</td>\n",
       "      <td>@RoastSmith_ I used play Fortnite a lot on my ...</td>\n",
       "      <td>@RoastSmith_ I used play Fortnite a lot on my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1548020151814987779</td>\n",
       "      <td>1640202097772601345</td>\n",
       "      <td>1640202097772601345</td>\n",
       "      <td>['1640202097772601345']</td>\n",
       "      <td>RT @Somali_ICS: If it wasn't for #Tiktok there...</td>\n",
       "      <td>2023-03-27T04:00:28.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Somali_ICS</td>\n",
       "      <td>If it wasn't for #Tiktok there would've been ...</td>\n",
       "      <td>If it wasn't for #Tiktok there would've been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27968588</td>\n",
       "      <td>1640194922799144961</td>\n",
       "      <td>1640194922799144961</td>\n",
       "      <td>['1640194922799144961']</td>\n",
       "      <td>@NYCMayor @ericadamsfornyc time to change cour...</td>\n",
       "      <td>2023-03-27T03:31:57.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vliscony</td>\n",
       "      <td>@NYCMayor @ericadamsfornyc time to change cour...</td>\n",
       "      <td>@NYCMayor @ericadamsfornyc time to change cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1428236322838220802</td>\n",
       "      <td>1640190517093990400</td>\n",
       "      <td>1640190517093990400</td>\n",
       "      <td>['1640190517093990400']</td>\n",
       "      <td>Feeling in the dumps because of lockdown? \\nHe...</td>\n",
       "      <td>2023-03-27T03:14:27.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CovidHelpBot</td>\n",
       "      <td>Feeling in the dumps because of lockdown? \\nHe...</td>\n",
       "      <td>Feeling in the dumps because of lockdown? \\nHe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>1428236322838220802</td>\n",
       "      <td>1637683999882760192</td>\n",
       "      <td>1637683999882760192</td>\n",
       "      <td>['1637683999882760192']</td>\n",
       "      <td>Feeling unhappy because of lockdown? \\nHere ar...</td>\n",
       "      <td>2023-03-20T05:14:26.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CovidHelpBot</td>\n",
       "      <td>Feeling unhappy because of lockdown? \\nHere ar...</td>\n",
       "      <td>Feeling unhappy because of lockdown? \\nHere ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>1590162549525422080</td>\n",
       "      <td>1637617552351215616</td>\n",
       "      <td>1637683256580616192</td>\n",
       "      <td>['1637683256580616192']</td>\n",
       "      <td>@TRyanGregory Lockdowns make billionaires more...</td>\n",
       "      <td>2023-03-20T05:11:29.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>InfiniteKB_Com</td>\n",
       "      <td>@TRyanGregory Lockdowns make billionaires more...</td>\n",
       "      <td>@TRyanGregory Lockdowns make billionaires more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>85601740</td>\n",
       "      <td>1637680366268841984</td>\n",
       "      <td>1637680366268841984</td>\n",
       "      <td>['1637680366268841984']</td>\n",
       "      <td>Of not passing: homelessness, addiction, menta...</td>\n",
       "      <td>2023-03-20T05:00:00.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Prison_Health</td>\n",
       "      <td>Of not passing: homelessness, addiction, menta...</td>\n",
       "      <td>Of not passing: homelessness, addiction, menta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>1557364662546714626</td>\n",
       "      <td>1637561236274528257</td>\n",
       "      <td>1637678230713909250</td>\n",
       "      <td>['1637678230713909250']</td>\n",
       "      <td>@olaadun @Samuelo84500495 @Dawa911 No, they wo...</td>\n",
       "      <td>2023-03-20T04:51:31.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ElliotLinksync</td>\n",
       "      <td>@olaadun @Samuelo84500495 @Dawa911 No, they wo...</td>\n",
       "      <td>@olaadun @Samuelo84500495 @Dawa911 No, they wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>1615335369150726145</td>\n",
       "      <td>1637670768518795264</td>\n",
       "      <td>1637670768518795264</td>\n",
       "      <td>['1637670768518795264']</td>\n",
       "      <td>RT @VaxFreeSperm: Eddie Griffin speaks the tru...</td>\n",
       "      <td>2023-03-20T04:21:52.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vi08696277</td>\n",
       "      <td>Eddie Griffin speaks the truth about Sheep an...</td>\n",
       "      <td>Eddie Griffin speaks the truth about Sheep an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3750 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                author_id      conversation_id                   id  \\\n",
       "0               495071269  1640204550631043072  1640204550631043072   \n",
       "1     1564131612505149440  1639919857012469760  1640204157683417089   \n",
       "2     1548020151814987779  1640202097772601345  1640202097772601345   \n",
       "3                27968588  1640194922799144961  1640194922799144961   \n",
       "4     1428236322838220802  1640190517093990400  1640190517093990400   \n",
       "...                   ...                  ...                  ...   \n",
       "3745  1428236322838220802  1637683999882760192  1637683999882760192   \n",
       "3746  1590162549525422080  1637617552351215616  1637683256580616192   \n",
       "3747             85601740  1637680366268841984  1637680366268841984   \n",
       "3748  1557364662546714626  1637561236274528257  1637678230713909250   \n",
       "3749  1615335369150726145  1637670768518795264  1637670768518795264   \n",
       "\n",
       "       edit_history_tweet_ids  \\\n",
       "0     ['1640204550631043072']   \n",
       "1     ['1640204157683417089']   \n",
       "2     ['1640202097772601345']   \n",
       "3     ['1640194922799144961']   \n",
       "4     ['1640190517093990400']   \n",
       "...                       ...   \n",
       "3745  ['1637683999882760192']   \n",
       "3746  ['1637683256580616192']   \n",
       "3747  ['1637680366268841984']   \n",
       "3748  ['1637678230713909250']   \n",
       "3749  ['1637670768518795264']   \n",
       "\n",
       "                                                   text  \\\n",
       "0     RT @Chris_EvansMP: Today marks three years sin...   \n",
       "1     @RoastSmith_ I used play Fortnite a lot on my ...   \n",
       "2     RT @Somali_ICS: If it wasn't for #Tiktok there...   \n",
       "3     @NYCMayor @ericadamsfornyc time to change cour...   \n",
       "4     Feeling in the dumps because of lockdown? \\nHe...   \n",
       "...                                                 ...   \n",
       "3745  Feeling unhappy because of lockdown? \\nHere ar...   \n",
       "3746  @TRyanGregory Lockdowns make billionaires more...   \n",
       "3747  Of not passing: homelessness, addiction, menta...   \n",
       "3748  @olaadun @Samuelo84500495 @Dawa911 No, they wo...   \n",
       "3749  RT @VaxFreeSperm: Eddie Griffin speaks the tru...   \n",
       "\n",
       "                    created_at withheld.copyright withheld.country_codes  \\\n",
       "0     2023-03-27T04:10:13.000Z                NaN                    NaN   \n",
       "1     2023-03-27T04:08:39.000Z                NaN                    NaN   \n",
       "2     2023-03-27T04:00:28.000Z                NaN                    NaN   \n",
       "3     2023-03-27T03:31:57.000Z                NaN                    NaN   \n",
       "4     2023-03-27T03:14:27.000Z                NaN                    NaN   \n",
       "...                        ...                ...                    ...   \n",
       "3745  2023-03-20T05:14:26.000Z                NaN                    NaN   \n",
       "3746  2023-03-20T05:11:29.000Z                NaN                    NaN   \n",
       "3747  2023-03-20T05:00:00.000Z                NaN                    NaN   \n",
       "3748  2023-03-20T04:51:31.000Z                NaN                    NaN   \n",
       "3749  2023-03-20T04:21:52.000Z                NaN                    NaN   \n",
       "\n",
       "         author_name                                       orginal_text  \\\n",
       "0      BarnettElaine   Today marks three years since the UK went int...   \n",
       "1           tanseus1  @RoastSmith_ I used play Fortnite a lot on my ...   \n",
       "2         Somali_ICS   If it wasn't for #Tiktok there would've been ...   \n",
       "3           vliscony  @NYCMayor @ericadamsfornyc time to change cour...   \n",
       "4       CovidHelpBot  Feeling in the dumps because of lockdown? \\nHe...   \n",
       "...              ...                                                ...   \n",
       "3745    CovidHelpBot  Feeling unhappy because of lockdown? \\nHere ar...   \n",
       "3746  InfiniteKB_Com  @TRyanGregory Lockdowns make billionaires more...   \n",
       "3747   Prison_Health  Of not passing: homelessness, addiction, menta...   \n",
       "3748  ElliotLinksync  @olaadun @Samuelo84500495 @Dawa911 No, they wo...   \n",
       "3749      Vi08696277   Eddie Griffin speaks the truth about Sheep an...   \n",
       "\n",
       "                                          original_text  \n",
       "0      Today marks three years since the UK went int...  \n",
       "1     @RoastSmith_ I used play Fortnite a lot on my ...  \n",
       "2      If it wasn't for #Tiktok there would've been ...  \n",
       "3     @NYCMayor @ericadamsfornyc time to change cour...  \n",
       "4     Feeling in the dumps because of lockdown? \\nHe...  \n",
       "...                                                 ...  \n",
       "3745  Feeling unhappy because of lockdown? \\nHere ar...  \n",
       "3746  @TRyanGregory Lockdowns make billionaires more...  \n",
       "3747  Of not passing: homelessness, addiction, menta...  \n",
       "3748  @olaadun @Samuelo84500495 @Dawa911 No, they wo...  \n",
       "3749   Eddie Griffin speaks the truth about Sheep an...  \n",
       "\n",
       "[3750 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_lockdown_df = pd.read_csv(folder_path + file1)\n",
    "twitter_lockdown_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9ffd0",
   "metadata": {},
   "source": [
    "<h3>Cleaning Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "403f1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_user_mentions(text):\n",
    "    return re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_digit_strings(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    remove_chars = '[0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "    return re.sub(remove_chars, ' ', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    result_text = text\n",
    "    result_text = remove_user_mentions(result_text)\n",
    "    result_text = remove_links(result_text)\n",
    "    result_text = remove_digit_strings(result_text)\n",
    "    result_text = remove_special_chars(result_text)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f23c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Clean text, and check for empty strings / strings containing only whitespace\n",
    "'''\n",
    "texts = twitter_lockdown_df[\"text\"].tolist()\n",
    "for i in range(len(texts)):\n",
    "    texts[i] = clean_text(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d831ff09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750\n",
      "RT   Today marks three years since the UK went into lockdown   \n",
      "\n",
      "On this National Day of Reflection  I visited the  \n",
      " I used play Fortnite a lot on my Android in COVID   lockdown and Chapter  Midas revenge was the best one\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c00c96",
   "metadata": {},
   "source": [
    "<h3>Tokenizing Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0d69954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "exclude_words = stop_words\n",
    "\n",
    "#exclude common words \n",
    "exclude_words_extra = [\"RT\",\"still\",\"covid\",\"lockdown\", \"pandemic\",\"get\",\"go\",\"im\",\"ive\",\"would\",\"one\",\"also\",\"to\"]\n",
    "\n",
    "exclude_words.extend(exclude_words_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d924d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63ef684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'today', 'marks', 'three', 'years', 'since', 'the', 'uk', 'went_into', 'lockdown', 'on', 'this', 'national', 'day', 'of', 'reflection', 'visited', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf98d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in exclude_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "497dc076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['today', 'mark', 'year', 'go', 'lockdown', 'national', 'day', 'reflection', 'visit']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b12c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('day', 1),\n",
       "  ('go', 1),\n",
       "  ('lockdown', 1),\n",
       "  ('mark', 1),\n",
       "  ('national', 1),\n",
       "  ('reflection', 1),\n",
       "  ('today', 1),\n",
       "  ('visit', 1),\n",
       "  ('year', 1)]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc119f85",
   "metadata": {},
   "source": [
    "<h3>LDA Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d410b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "# supporting function 2\n",
    "def compute_perplexity_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    perplexity_score = lda_model.log_perplexity(corpus_sets[i])\n",
    "    \n",
    "    return perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               # gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "corpus_title = ['100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': [],\n",
    "                 'Perplexity': []\n",
    "                }\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=271)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    p = compute_perplexity_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    model_results['Perplexity'].append(p)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('./twitter_lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df510f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a01621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b74b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = lda_model\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6161b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, atopic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summaries = []\n",
    "print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "for i in range(4):\n",
    "    print('Topic '+str(i)+' |---------------------\\n')\n",
    "    tmp = explore_topic(lda_model,topic_number=i, topn=10, output=True )\n",
    "#     print tmp[:5]\n",
    "    topic_summaries += [tmp[:5]]\n",
    "    print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
