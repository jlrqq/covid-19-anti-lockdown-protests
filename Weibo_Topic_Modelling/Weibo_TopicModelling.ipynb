{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "23fb1a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../Weibo_Data/\"\n",
    "\n",
    "visuals_output_path = \"./visuals/\"\n",
    "output_path = \"./output/\"\n",
    "\n",
    "file1 = \"weibo_封城+疫情.csv\"\n",
    "file2 = \"weibo_封城.csv\"\n",
    "file3 = \"weibo_疫情.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "531f95fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\lindy\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (58.0.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.7.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: funcy in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.18)\n",
      "Requirement already satisfied: pandas>=1.3.4 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.2)\n",
      "Requirement already satisfied: numexpr in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from pandas>=1.3.4->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.3.4->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lindy\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f17a0642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lindy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords') #download if don't have yet\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer # tokenize words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "beb7bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in files\n",
    "df1 = pd.read_csv(folder_path + file1)\n",
    "df2 = pd.read_csv(folder_path + file2)\n",
    "df3 = pd.read_csv(folder_path + file3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973bfd4",
   "metadata": {},
   "source": [
    "<h1>Cleaning Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d5ae8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = df1[\"text\"].tolist()\n",
    "list2 = df2[\"text\"].tolist()\n",
    "list3 = df3[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "063aa860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_user_mentions(text):\n",
    "    return re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_digit_strings(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    remove_chars = '[0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "    return re.sub(remove_chars, ' ', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    result_text = text\n",
    "    result_text = remove_user_mentions(result_text)\n",
    "    result_text = remove_links(result_text)\n",
    "    result_text = remove_digit_strings(result_text)\n",
    "    result_text = remove_special_chars(result_text)\n",
    "    result_text = result_text.lower()\n",
    "    return result_text\n",
    "\n",
    "def remove_cn_chars(text):\n",
    "    result_text = re.sub(r'([\\u4e00-\\u9fff]+', '', text)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ef74b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list1)):\n",
    "#     list1[i] = remove_cn_chars(list1[i])\n",
    "    list1[i] = clean_text(list1[i])\n",
    "\n",
    "for i in range(len(list2)):\n",
    "#     list2[i] = remove_cn_chars(list2[i])\n",
    "    list2[i] = clean_text(list2[i])\n",
    "\n",
    "for i in range(len(list3)):\n",
    "#     list3[i] = remove_cn_chars(list3[i])\n",
    "    list3[i] = clean_text(list3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d6c5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"cleaned_text\"] = list1\n",
    "df2[\"cleaned_text\"] = list2\n",
    "df3[\"cleaned_text\"] = list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b97d515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of file name to associated data\n",
    "\n",
    "weibo = {}\n",
    "keys = [\"weibo_封城+疫情\", \"weibo_封城\", \"weibo_疫情\"]\n",
    "for key in keys:\n",
    "    weibo[key] = {}\n",
    "    \n",
    "weibo[\"weibo_封城+疫情\"][\"text\"] = df1[\"cleaned_text\"].tolist()\n",
    "weibo[\"weibo_封城\"][\"text\"] = df2[\"cleaned_text\"].tolist()\n",
    "weibo[\"weibo_疫情\"][\"text\"] = df3[\"cleaned_text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28371fae",
   "metadata": {},
   "source": [
    "<h1>Tokenizing Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3f2f7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get custom stopwords\n",
    "content = []\n",
    "f = open(\"./stopwords.txt\", encoding = 'utf-8')\n",
    "# perform file operations\n",
    "for line in f:\n",
    "    content.append(line)\n",
    "f.close()\n",
    "\n",
    "custom_stopwords = []\n",
    "for line in content:\n",
    "    wordlist = line.split(\",\")\n",
    "    for word in wordlist:\n",
    "        custom_stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ef2482b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "exclude_words = stop_words\n",
    "\n",
    "exclude_words_extra = [\"wuhan\",\"covid\",\"coronavirus\",\"lockdown\",\"lockdo\",\"pandemic\",\"let\",\"get\",\"ago\",\"go\",\"im\",\"ive\",\"would\",\"one\",\"also\",\"to\",\"tag\"]\n",
    "\n",
    "# Exclude custom stopwords\n",
    "exclude_words.extend(custom_stopwords)\n",
    "exclude_words.extend(exclude_words_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7dbf9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "for w in weibo.keys():\n",
    "    weibo[w][\"data_words\"] = list(sent_to_words(weibo[w][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b3eda1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_trigram_models(w):\n",
    "    data_words = weibo[w][\"data_words\"]\n",
    "    \n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "    \n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    weibo[w][\"bigram\"] = bigram\n",
    "    weibo[w][\"trigram\"] = trigram\n",
    "    weibo[w][\"bigram_mod\"] = bigram_mod\n",
    "    weibo[w][\"trigram_mod\"] = trigram_mod\n",
    "\n",
    "for w in weibo.keys():\n",
    "    bigram_trigram_models(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "82a5cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in exclude_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts, bigram_mod, trigram_mod):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eff1c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined(w):\n",
    "    data_words = weibo[w][\"data_words\"]\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "    weibo[w][\"data_words_nostops\"] = data_words_nostops \n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops, weibo[w][\"bigram_mod\"])\n",
    "    weibo[w][\"data_words_bigrams\"] = data_words_bigrams\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    weibo[w][\"data_lemmatized\"] = data_lemmatized\n",
    "\n",
    "for w in weibo.keys():\n",
    "    combined(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "28b336b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'data_words', 'bigram', 'trigram', 'bigram_mod', 'trigram_mod', 'data_words_nostops', 'data_words_bigrams', 'data_lemmatized'])\n"
     ]
    }
   ],
   "source": [
    "print(weibo[list(weibo.keys())[0]].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a15be734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(w):\n",
    "    data_lemmatized = weibo[w][\"data_lemmatized\"]\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "    weibo[w][\"id2word\"] = id2word\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "    weibo[w][\"texts\"] = texts\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    weibo[w][\"corpus\"] = corpus\n",
    "\n",
    "    # Human readable format of corpus (term-frequency)\n",
    "    weibo[w][\"corpus_readable\"] = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "for w in weibo.keys():\n",
    "    tokenize(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f60ebf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('advance', 1), ('city', 3), ('discipline', 1), ('dog', 1), ('event', 1), ('example', 1), ('fall', 1), ('go', 1), ('grow', 1), ('honest', 1), ('maybe', 1), ('myth', 1), ('prepare', 1), ('speak', 1), ('strange', 1), ('student', 1), ('subject', 1), ('today', 1), ('yesterday', 1)]]\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(weibo[list(weibo.keys())[index]][\"corpus_readable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a37d8",
   "metadata": {},
   "source": [
    "<h1>LDA Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "992c74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "# supporting function 2\n",
    "def compute_perplexity_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    perplexity_score = lda_model.log_perplexity(corpus_sets[i])\n",
    "    \n",
    "    return perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fee34238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(w):\n",
    "    corpus = weibo[w][\"corpus\"]\n",
    "    id2word = weibo[w][\"id2word\"]\n",
    "\n",
    "    # Build LDA model\n",
    "    num_topics = 4\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=num_topics, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "    weibo[w][\"lda_model\"] = lda_model\n",
    "\n",
    "for w in weibo.keys():\n",
    "    lda_model(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2cb03d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel<num_terms=2768, num_topics=4, decay=0.5, chunksize=100>\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(weibo[list(weibo.keys())[index]][\"lda_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "065730d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.025*\"city\" + 0.016*\"text\" + 0.015*\"full\" + 0.013*\"people\" + 0.012*\"many\" '\n",
      "  '+ 0.011*\"number\" + 0.011*\"impact\" + 0.010*\"spread\" + 0.010*\"outbreak\" + '\n",
      "  '0.009*\"close\"'),\n",
      " (1,\n",
      "  '0.034*\"day\" + 0.031*\"year\" + 0.029*\"city\" + 0.026*\"full\" + 0.025*\"time\" + '\n",
      "  '0.021*\"close\" + 0.014*\"go\" + 0.014*\"text\" + 0.012*\"home\" + 0.012*\"today\"'),\n",
      " (2,\n",
      "  '0.042*\"people\" + 0.018*\"country\" + 0.016*\"full\" + 0.015*\"text\" + '\n",
      "  '0.014*\"city\" + 0.013*\"medical\" + 0.013*\"make\" + 0.013*\"control\" + '\n",
      "  '0.013*\"fight\" + 0.011*\"life\"'),\n",
      " (3,\n",
      "  '0.029*\"new\" + 0.020*\"country\" + 0.016*\"director\" + 0.015*\"express\" + '\n",
      "  '0.013*\"confirm\" + 0.010*\"accord\" + 0.010*\"week\" + 0.009*\"quarantine\" + '\n",
      "  '0.009*\"later\" + 0.009*\"text\"')]\n",
      "[(0,\n",
      "  '0.014*\"soon\" + 0.012*\"sell\" + 0.011*\"wave\" + 0.009*\"lie\" + 0.008*\"hand\" + '\n",
      "  '0.008*\"strain\" + 0.008*\"everywhere\" + 0.008*\"die\" + 0.008*\"know\" + '\n",
      "  '0.008*\"spread\"'),\n",
      " (1,\n",
      "  '0.016*\"people\" + 0.014*\"high\" + 0.013*\"country\" + 0.013*\"less\" + '\n",
      "  '0.010*\"text\" + 0.009*\"past\" + 0.007*\"number\" + 0.007*\"policy\" + '\n",
      "  '0.007*\"medium\" + 0.007*\"much\"'),\n",
      " (2,\n",
      "  '0.013*\"outbreak\" + 0.010*\"lot\" + 0.010*\"hard\" + 0.009*\"need\" + 0.008*\"read\" '\n",
      "  '+ 0.008*\"write\" + 0.008*\"kind\" + 0.008*\"little\" + 0.008*\"text\" + '\n",
      "  '0.007*\"afraid\"'),\n",
      " (3,\n",
      "  '0.041*\"city\" + 0.039*\"year\" + 0.029*\"full\" + 0.023*\"day\" + 0.022*\"time\" + '\n",
      "  '0.021*\"last\" + 0.020*\"close\" + 0.016*\"people\" + 0.014*\"text\" + 0.013*\"go\"')]\n",
      "[(0,\n",
      "  '0.018*\"world\" + 0.016*\"era\" + 0.014*\"text\" + 0.014*\"know\" + 0.013*\"end\" + '\n",
      "  '0.011*\"life\" + 0.010*\"together\" + 0.010*\"full\" + 0.009*\"learn\" + '\n",
      "  '0.008*\"never\"'),\n",
      " (1,\n",
      "  '0.020*\"full\" + 0.019*\"text\" + 0.019*\"people\" + 0.015*\"wear\" + '\n",
      "  '0.013*\"government\" + 0.011*\"many\" + 0.010*\"front\" + 0.010*\"love\" + '\n",
      "  '0.010*\"new\" + 0.009*\"control\"'),\n",
      " (2,\n",
      "  '0.033*\"time\" + 0.021*\"go\" + 0.021*\"day\" + 0.019*\"year\" + 0.019*\"full\" + '\n",
      "  '0.017*\"last\" + 0.017*\"see\" + 0.016*\"people\" + 0.013*\"today\" + 0.012*\"work\"'),\n",
      " (3,\n",
      "  '0.034*\"full\" + 0.031*\"text\" + 0.016*\"need\" + 0.015*\"year\" + 0.011*\"number\" '\n",
      "  '+ 0.010*\"begin\" + 0.010*\"provide\" + 0.010*\"morning\" + 0.009*\"wearing_mask\" '\n",
      "  '+ 0.008*\"yuan\"')]\n"
     ]
    }
   ],
   "source": [
    "def keyword_in_topics(w):\n",
    "    corpus = weibo[w][\"corpus\"]\n",
    "    lda_model = weibo[w][\"lda_model\"]\n",
    "\n",
    "    # Print the Keyword in the 10 topics\n",
    "    pprint(lda_model.print_topics())\n",
    "    doc_lda = lda_model[corpus]\n",
    "    weibo[w][\"doc_lda\"] = doc_lda\n",
    "\n",
    "for w in weibo.keys():\n",
    "    keyword_in_topics(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6d26e4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x000001C23BEBFCD0>\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(weibo[list(weibo.keys())[index]][\"doc_lda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9038b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_coherence(w):\n",
    "    corpus = weibo[w][\"corpus\"]\n",
    "    data_lemmatized = weibo[w][\"data_lemmatized\"]\n",
    "    id2word = weibo[w][\"id2word\"]\n",
    "    lda_model = weibo[w][\"lda_model\"]\n",
    "\n",
    "    # Compute Perplexity\n",
    "    weibo[w][\"perplexity\"] = lda_model.log_perplexity(corpus)\n",
    "    # print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "    weibo[w][\"coherence_model_lda\"] = coherence_model_lda\n",
    "    weibo[w][\"coherence_lda\"] = coherence_lda\n",
    "\n",
    "    # print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "for w in weibo.keys():\n",
    "    perplexity_coherence(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "861104a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.996089070469202\n",
      "0.43052963952756484\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(weibo[list(weibo.keys())[index]][\"perplexity\"])\n",
    "print(weibo[list(weibo.keys())[index]][\"coherence_lda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6df5674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_model(w): \n",
    "    lda_model = weibo[w][\"lda_model\"]\n",
    "\n",
    "    optimal_model = lda_model\n",
    "    model_topics = optimal_model.show_topics(formatted=False)\n",
    "\n",
    "    weibo[w][\"model_topics\"] = model_topics\n",
    "\n",
    "for w in weibo.keys():\n",
    "    optimal_model(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f66d6fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [('soon', 0.013631528), ('sell', 0.011673953), ('wave', 0.010919605), ('lie', 0.009306657), ('hand', 0.008355316), ('strain', 0.008177674), ('everywhere', 0.007935251), ('die', 0.0077032903), ('know', 0.007525805), ('spread', 0.00751747)]), (1, [('people', 0.01639532), ('high', 0.013906687), ('country', 0.013197607), ('less', 0.012678805), ('text', 0.010279735), ('past', 0.009227668), ('number', 0.0072286665), ('policy', 0.006835826), ('medium', 0.0067634718), ('much', 0.006626914)]), (2, [('outbreak', 0.012870387), ('lot', 0.010422628), ('hard', 0.0101859765), ('need', 0.008714052), ('read', 0.008118482), ('write', 0.007911781), ('kind', 0.007873071), ('little', 0.00784374), ('text', 0.007790091), ('afraid', 0.0073796595)]), (3, [('city', 0.041282814), ('year', 0.038655777), ('full', 0.028891843), ('day', 0.0234722), ('time', 0.022343297), ('last', 0.021189958), ('close', 0.019520547), ('people', 0.016406493), ('text', 0.014261148), ('go', 0.012534313)])]\n"
     ]
    }
   ],
   "source": [
    "index = 1   # change this to see diff output\n",
    "print(weibo[list(weibo.keys())[index]][\"model_topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0a818cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, atopic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a5e8e0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "weibo_封城+疫情\n",
      "Topic 0 |---------------------\n",
      "\n",
      "city                 0.025\n",
      "text                 0.016\n",
      "full                 0.015\n",
      "people               0.013\n",
      "many                 0.012\n",
      "number               0.011\n",
      "impact               0.011\n",
      "spread               0.010\n",
      "outbreak             0.010\n",
      "close                0.009\n",
      "Topic 1 |---------------------\n",
      "\n",
      "day                  0.034\n",
      "year                 0.031\n",
      "city                 0.029\n",
      "full                 0.026\n",
      "time                 0.025\n",
      "close                0.021\n",
      "go                   0.014\n",
      "text                 0.014\n",
      "home                 0.012\n",
      "today                0.012\n",
      "Topic 2 |---------------------\n",
      "\n",
      "people               0.042\n",
      "country              0.018\n",
      "full                 0.016\n",
      "text                 0.015\n",
      "city                 0.014\n",
      "medical              0.013\n",
      "make                 0.013\n",
      "control              0.013\n",
      "fight                0.013\n",
      "life                 0.011\n",
      "Topic 3 |---------------------\n",
      "\n",
      "new                  0.029\n",
      "country              0.020\n",
      "director             0.016\n",
      "express              0.015\n",
      "confirm              0.013\n",
      "accord               0.010\n",
      "week                 0.010\n",
      "quarantine           0.009\n",
      "later                0.009\n",
      "text                 0.009\n",
      "term                 frequency\n",
      "\n",
      "weibo_封城\n",
      "Topic 0 |---------------------\n",
      "\n",
      "soon                 0.014\n",
      "sell                 0.012\n",
      "wave                 0.011\n",
      "lie                  0.009\n",
      "hand                 0.008\n",
      "strain               0.008\n",
      "everywhere           0.008\n",
      "die                  0.008\n",
      "know                 0.008\n",
      "spread               0.008\n",
      "Topic 1 |---------------------\n",
      "\n",
      "people               0.016\n",
      "high                 0.014\n",
      "country              0.013\n",
      "less                 0.013\n",
      "text                 0.010\n",
      "past                 0.009\n",
      "number               0.007\n",
      "policy               0.007\n",
      "medium               0.007\n",
      "much                 0.007\n",
      "Topic 2 |---------------------\n",
      "\n",
      "outbreak             0.013\n",
      "lot                  0.010\n",
      "hard                 0.010\n",
      "need                 0.009\n",
      "read                 0.008\n",
      "write                0.008\n",
      "kind                 0.008\n",
      "little               0.008\n",
      "text                 0.008\n",
      "afraid               0.007\n",
      "Topic 3 |---------------------\n",
      "\n",
      "city                 0.041\n",
      "year                 0.039\n",
      "full                 0.029\n",
      "day                  0.023\n",
      "time                 0.022\n",
      "last                 0.021\n",
      "close                0.020\n",
      "people               0.016\n",
      "text                 0.014\n",
      "go                   0.013\n",
      "term                 frequency\n",
      "\n",
      "weibo_疫情\n",
      "Topic 0 |---------------------\n",
      "\n",
      "world                0.018\n",
      "era                  0.016\n",
      "text                 0.014\n",
      "know                 0.014\n",
      "end                  0.013\n",
      "life                 0.011\n",
      "together             0.010\n",
      "full                 0.010\n",
      "learn                0.009\n",
      "never                0.008\n",
      "Topic 1 |---------------------\n",
      "\n",
      "full                 0.020\n",
      "text                 0.019\n",
      "people               0.019\n",
      "wear                 0.015\n",
      "government           0.013\n",
      "many                 0.011\n",
      "front                0.010\n",
      "love                 0.010\n",
      "new                  0.010\n",
      "control              0.009\n",
      "Topic 2 |---------------------\n",
      "\n",
      "time                 0.033\n",
      "go                   0.021\n",
      "day                  0.021\n",
      "year                 0.019\n",
      "full                 0.019\n",
      "last                 0.017\n",
      "see                  0.017\n",
      "people               0.016\n",
      "today                0.013\n",
      "work                 0.012\n",
      "Topic 3 |---------------------\n",
      "\n",
      "full                 0.034\n",
      "text                 0.031\n",
      "need                 0.016\n",
      "year                 0.015\n",
      "number               0.011\n",
      "begin                0.010\n",
      "provide              0.010\n",
      "morning              0.010\n",
      "wearing_mask         0.009\n",
      "yuan                 0.008\n"
     ]
    }
   ],
   "source": [
    "def explore_topic_by_title(w):\n",
    "    lda_model = weibo[w][\"lda_model\"]\n",
    "    topic_summaries = []\n",
    "\n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "    print(w)\n",
    "    for i in range(4):\n",
    "        print('Topic '+str(i)+' |---------------------\\n')\n",
    "        tmp = explore_topic(lda_model,topic_number=i, topn=10, output=True )\n",
    "        #     print tmp[:5]\n",
    "        topic_summaries += [tmp[:5]]\n",
    "        # print\n",
    "\n",
    "    weibo[w][\"topic_summaries\"] = topic_summaries\n",
    "\n",
    "for w in weibo.keys():\n",
    "    explore_topic_by_title(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3db731",
   "metadata": {},
   "source": [
    "<h1>Data Output - Dominant Topic</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f4d234ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "for w in weibo.keys():\n",
    "    lda_model = weibo[w][\"lda_model\"]\n",
    "    corpus = weibo[w][\"corpus\"]\n",
    "    texts = weibo[w][\"texts\"]\n",
    "    \n",
    "    df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, texts)\n",
    "    \n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    \n",
    "    weibo[w][\"df_dominant_topic\"] = df_dominant_topic\n",
    "    \n",
    "    df_dominant_topic.to_csv(output_path + w + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9589b9b",
   "metadata": {},
   "source": [
    "<h1>Data Visualization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e904c0",
   "metadata": {},
   "source": [
    "<h3>PyLDAVis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f45cea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lindy\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "# Visualize the topics\n",
    "for w in weibo.keys():\n",
    "    lda_model = weibo[w][\"lda_model\"]\n",
    "    corpus = weibo[w][\"corpus\"]\n",
    "    id2word = weibo[w][\"id2word\"]\n",
    "\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    \n",
    "    file_path = visuals_output_path + w + '.html'\n",
    "    pyLDAvis.save_html(LDAvis_prepared, file_path)\n",
    "    \n",
    "    with open(file_path, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        \n",
    "        if (w == \"weibo_封城+疫情\"):\n",
    "            f.write(\"<h1>\" + \"weibo_closure+epidemic\" + \"</h1>\" + '\\n' + content)\n",
    "        elif (w == \"weibo_封城\"):\n",
    "            f.write(\"<h1>\" + \"weibo_closure\" + \"</h1>\" + '\\n' + content)\n",
    "        elif (w == \"weibo_疫情\"):\n",
    "            f.write(\"<h1>\" + \"weibo_epidemic\" + \"</h1>\" + '\\n' + content)\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
